\chapter{Deep Reinforcement Learning}
Deep Learning describes a family of learning techniques
that at their core are about learning representations of data
in a hierarchical fashion.
It replaces the need for handcrafted feature
and indeed works in an entirely unsupervised or at least
semi-supervised fashion.
In this way,
it is often deployed as end-to-end learning
because it can learn its own higher-level abstractions
from raw data.

The core algorithm in deep learning is backpropagation
(described in detail in \ref{sec:backprop})
which describes how a learning model should update its parameters
in response to a discrepancy between learned output
and example output.

\paragraph{}
While the notion of stacking layers of units
to create multi-tiered artificial neural networks
has been around for a few decades already,
% TODO ref first Ann?
the recent surge in computing power
has enabled these conceptual models
to be actually built large enough
to explore their full potential.

Next to fully connected neural networks,
there are a few other types of networks of interest
that I will explore.
One such is the Convolutional Neural Network
which I will describe in more detail in
section \ref{sec:cnn}.
This type especially has benefitted from
advances in technology because of their highly
parallel nature which effectively allows them to be run
on consumer graphic hardware which is ever-improving
and becoming more accessible.

\paragraph{}
The areas that have benefited most from
this branch of machine learning
include without doubt
image recognition and speech recognition,
both problems with extremely noisy real-world data
that were typically tackled
by hand-crafting higher level features
and learning from those,
yet it is far from limited to these.
Deep learning has been extensively and successfully
applied to reinforcement learning as well,
the main topic of interest in this thesis.

\paragraph{}
The rest of this chapter will start by introducing
core deep learning conceps,
then proceed to applying deep learning
to the reinforcement learning case.
Sections \ref{sec:cnn} and \ref{sec:rnn}
concern themselves with general deep learning techniques,
i.e. not specific to the reinforcement learning.

% TODO add refs to later sections


\section{Convolutional Neural Networks}
\label{sec:cnn}
The core to deep learning is that representations
of data can be learned in an unsupervised manner.
Convolutional networks are able to learn features
from raw input without any human intervention.
Stacking convolutional layers even allows
hierarchical features to be learned.
This way the initial layer could function
as an edge detector
whereas deeper layers
would learn higher-level abstractions
such as for example facial features
in the case of image recognition.

Another core property of convolutional layers is that,
unlike a regular fully connected neural network,
it can recognize the same features
regardless of the location of the feature in the input.
A regular fully connected network would need to have
its weights in each of the possible locations trained to recognize the same feature
which is wasteful in both space and computational requirements.

\paragraph{}
Convolutional networks are strongly inspired
by the animal visual cortex which contains two basic cell types.
Simple cells activate respond most strongly to edge patterns;
they function as edge-detectors and have a small receptive field.
Complex cells on the other hands have a larger receptive field
and are also spatially invariant to pattern location.

An early predecessor of the convolutional network
that tried to capture these concepts
is the neocognitron
\parencite{Fukushima1980}
which differs mostly in that convolutional networks
share weights across several positions in the input,
as I will explain further down.
The architecture I will explain in the following section
is based on the famous LeNet-5,
designed by
\citeauthor{LeCun1998}
(\citeyear{LeCun1998})
to successfully recognize hand-written letters.

\section{Building Blocks}
\label{sec:building_blocks}

\subsection{Convolutional Layer}
The core building block for a convolutional network
is the convolutional layer.
For the remainder of this text it is easiest
to think of input as images, possibly
with a depth dimension (such as color).

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.7\linewidth]{conv_layer.png}
  \caption{Convolutional layer (blue) connected to an input layer.
    Highlighted are multiple filters connecting to a single input tile.
  }
  \label{fig:conv_layer}
\end{figure}

\paragraph{}
Let us start with the notion of a single, trainable neuron
in the convolutional layer.
It connects to only a small part of
the underlying input layer
as demonstrated in
Figure \ref{fig:conv_layer},
albeit through the whole depth.
This is the receptive field
as I described earlier.
Strongly connecting to only a small part of the input
is what allows convolutional networks
to exploit strong local correlations
that are inherently present
in natural images
but also in many other forms of
naturally occurring data.

\paragraph{}
This single neuron is part of a filter
that spans the whole input image,
it has neighbors that just like it
fully connect to neighboring patches of input.
One such a filter is now a flat 2D activation map
for a single feature.
This replication for a single filter
is what makes convolutional networks
insensitive to the location of a feature.

Of course we would like to have multiple features,
so we stack multiple of these filters on top of one another.
This stacking behavior of filters is described in
Figure \ref{fig:conv_layer},
where a single input patch is shown to correspond
to several filters.

\subsection{Parameter Sharing}
\label{sub:parameter_sharing}
As stated before, one feature corresponds to neurons across
the whole input by each corresponding to some patch of it.
This relies on the assumption that the feature could arise
anywhere in the input and would be useful to discover anywhere.
In order to actually compute the same feature,
we constrain the weights and bias of the neurons for a single feature
to be shared.

\subsection{Details}
\label{sub:details}
To actually generate the feature map
we \textit{convolve} the input with a linear filter,
add a bias and afterwards apply a non-linear function
such as a rectifier
(described in Section \ref{sec:relu}).
It is only because the weights are shared across a single feature
that this convolution is possible,
hence the name of the layer.

\paragraph{}
A single feature map $k$ in terms of its input $x$ could be computed as:

\begin{equation}
  h_k = tanh((W_k \cdot x) + b_k)
\end{equation}

It is also this operation that allows convolutional networks
to run so efficiently on parallel hardware,
making it the ideal candidate
for general purpose GPU computing.

\subsection{Tuning}
A single convolutional layer
has some hyperparameters that are both important
and hard to tweak.
Since learning a convolutional network
is still a rather slow endeavour,
it is best to start out with good estimates from the deep learning community.

\paragraph{}
The common way to build a convolutional network
from convolutional layers is to have
the layers closer to the input compute
few features,
computed over large receptive fields or tiles.
As the network grows deeper,
inputs to layers represent higher level features
and can thus be combined with less at a time;
intuitively, a few high-level features
can contain the same information of more
lower-level features.
Conversely, while there are only few
worthwhile raw features such as different types of edges,
there are probably more distinct higher-level features
that can be used to generate the final output.
Deeper layers should thus grow deeper yet slimmer spatially.

This setup typically results in a funnel-like structure
such as shown in Figure \ref{fig:conv_layer_funnel}.
The deepening occurs because of the increase in features
whereas the slimming usually occurs because multiple
outputs from an input layer correspond to only a single
neuron, spatially, in the next layer.
However, there are ways to train a neuron on a patch of input
yet still retain output size (again, spatially).
These I will describe below.

\begin{figure}[htpb]
  % TODO attribute to Hausknecht2015
  % TODO no seriously create new
  \centering
  \includegraphics[width=0.3\linewidth]{conv_layer_funnel.png}
  \caption{Convolutional layers are often stacked
    in a funnel-like manner,
    growing smaller spatially
    yet larger in the depth or feature dimension.
  }
  \label{fig:conv_layer_funnel}
\end{figure}

\paragraph{}
As noted, both the amount of features
as well as the size of the receptive field in a layer
play a huge role in the representational capacity of the layer.
Still, there are other ways to affect this.
\begin{description}
  \item[Stride]
    The stride for a convolutional layer determines the spacing
    between receptive fields, or tiles.
    A stride of 1 would have very strongly overlapping tiles
    and as a result larger spatial dimensions
    than a lower stride would have.
    A stride equal to the tile size
    would result in non-overlapping but touching tiles.
    An even larger one would, of course,
    result in unused input elements and is therefore
    rather unpopular in practice.
  \item[Padding]
    If not all input elements can be used
    because of the filter size or the combination
    of filter size and stride,
    one can choose to pad the input with zeroes
    to achieve a valid convolution.
\end{description}

\subsection{Pooling Layer}
An important yet simpler concept to
convolutional networks is pooling,
a non-linear downsampling of the input.
The most popular form is max pooling,
demonstrated in Figure \ref{fig:maxpool}.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.7\linewidth]{maxpool.jpg}
  \caption{Max pooling with a filter size of $2 \times 2$
    and stride $2$.
  }
  \label{fig:maxpool}
\end{figure}

\paragraph{}
Pooling reduces the spatial size
while retaining the depth
as the depth describes the amount of features.
The idea behind the concept
is that spatial location of a feature is less important
than the activation of the feature
(note again the importance of translation invariance),
along with the notion that features are most important
in relation to other features.
Pooling therefore preserves relative location of each feature.

\paragraph{}
Even though most convolutional networks
tended to have pooling layers,
if not occasionally in between convolutional layers
then often right after them,
in recent years in the literature
there has been a tendency to avoid pooling layers altogether.
\citeauthor{Springenberg2015}
(\citeyear{Springenberg2015})
suggest that pooling does not always improve performance
if the network already has enough capacity for the data at hand
and indeed advocate the use of convolutional layers
with larger strides
and even more convolutional layer
to make up for the loss in power.

\section{Recurrent Neural Networks}
\label{sec:rnn}
Some tasks are sequential in nature,
meaning one sample depends on a previous one,
rather than data being independently
drawn from some unknown underlying distribution.
Typical problems include speech and language.

Recurrent neural networks are especially suited
for these domains.
They process input sequences one step at a time,
maintaining learned hidden state which will then
affect future output.
% TODO mention recurring connection

\paragraph{}
It is no wonder this class of artificial neural networks
draws the eye of the reinforcement learner designer.
A problem with a pure Markov state space
has no need of a network useful for learning dependencies between different states,
since by definition every state on its own is sufficient enough of a representation
and includes all necesary past states in its description.
However, many reinforcement learning problems are not strictly Markovian
and are only reduced to Markov decision processes for convenience.
Since these problems do not adhere strictly to Markovness,
they could still benefit from RNN's.

Recall also from section \ref{sub:pomdp}
the class of Partially Observable Markov Decision Procceses (POMDP).
These are processes that simply lack all required information in
at least some state descriptions.
As a result this is the class of problems that benefit most
from recurrent neural networks
and as a result are of special interest in this thesis.

\subsection{Building Blocks and Training}
\label{sub:building_blocks_and_training}
As Recurrent Neural Networks form a class of networks,
I will go over the general approach taken instead of a specific version.

Whereas regular feed-forward networks contain only forward connections,
RNN's usually contain cycles allowing them to capture time dependencies.
The cycle allows on state to depend on a previous one,
making it the ideal setup for sequences.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\linewidth]{rnn.jpg}
  \caption{Recurrent Neural Network unfolded through time
  \parencite{Y.2015dl}.
  }
  \label{fig:rnn}
\end{figure}

At first, it might look daunting to train a network with recurrent connections
and as a result a recursive component in the error.
However, recurrent neural networks can simply be trained with gradient descent.
Figure \ref{fig:rnn} shows how a recurrent connection can be viewed
throughout three different time steps.
Seen in this rolled out fashion,
it simply becomes a very deep network with shared weights
and allows us to apply the well-known backpropagation algorithm.

\subsection{Long Short-Term Memory Networks}
\label{sub:lstm}
Regular recurrent neural networks can suffer from two problems during training:
\textit{vanishing gradient} and {exploding gradient}.
Highlighted first by
\citeauthor{Bengio1994} (\citeyear{Bengio1994}),
these two recurring problematic phenomena have long prevented
efficient traiffic of recurrent neural networks.

The \textit{exploding gradient} describes the norm of the gradient
increasing during training
because of explosive growth of the long-term components
which then far outweigh the short-term components.
The opposite phenomenon, \textit{vanishing gradient},
features more prominently in the literature.
It describes long term components that go exponentially fast to zero,
making it practically impossible to learn
long-term time dependencies of arbitrary length.

To deal with the exploding gradient,
\parencite{Pascanu2012}
suggest clipping the norm of the gradients
and
% TODO pls verify with Peter
\citeauthor{Graves2013} (\citeyear{Graves2013})
show that \textit{skip connections},
i.e. connections that `skip' a layer,
help mitigate the vanishing gradient problem
for deep networks.

\paragraph{}
An architecture especially good at storing and accessing information
is the \textit{Long Short-Term Memory},
introduced by \citeauthor{Hochreiter1997} (\citeyear{Hochreiter1997}).
% TODO eh you sure?
It remedies extreme gradients
by enforcing a constant error flow through special internal units.
It also contains gates that regulate
which inputs get remembered or even forgotten,
as well as gates that regulate when to output a remembered value.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\linewidth]{lstm.png}
  \caption{Long Short-Term Memory cell
  \parencite{Graves2013}.
  }
  \label{fig:lstm}
\end{figure}

\paragraph{}
The basic unit is a Long Short-Term Memory cell
which is displayed schematically in Figure \ref{fig:lstm}.
A cell contains three gates:
an input gate, forget gate and output gate.
Each gate uses an activation function,
often the sigmoid function.
Central to it all is the cell unit, the internal state,
which gets regulated by the gates
in a fashion described by their names;
one gate controls whether the internal state should be overridden,
the other whether it should be used in the output
and yet another whether the internal state should be forgotten.
The combination of these elements make the LSTM cell into
into a veritable piece of memory.

\paragraph{}
The relations between the different components are described as follows,
where $\sigma$ is the sigmoid activation function:

\begin{align}
  &i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1}+W_{ci}c_{t-1} + b_i) \\
  &f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1}+W_{cf}c_{t-1} + b_f) \\
  &o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1}+W_{co}c_{t-1} + b_o) \\
  &c_t = f_tc_{t-1}+i_t tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c) \\
  &h_t = o_t tanh(c_t)
\end{align}

The $W_{ij}$ notation denotes the weights associated
with the connection from unit $i$ to unit $j$.
Likewise, $b_k$ denotes the bias for unit $k$.

While the equations might look daunting at first glance,
they can all be given intuitive meaning.
The three gates behave similarly;
they all depend on the current cell input ($x_t$),
the previous output ($h_{t-1}$)
and previous internal cell state ($c_{t-1}$).
All combine these inputs using a linear computation
and become non-linear through the activation function.

The internal cell state is a combination
of a `forget component' that determines
how the previous cell state is carried over
along with a combination of the current input and the previous output,
weighted by the current input gate
which regulates how the input should be used.

It must be noted that the original LSTM proposal
did not contain a forget gate
and simply added unchanged cell state
back into the current update.
This cell state was then referred to as the Constant Error Carousel (CEC),
named so because it enforced a constant error
in order to mitigate the vanishing and exploding gradient problems.



\section{Misc, still needs a decent place}
\subsection{RMSProp}
\label{sub:rmsprop}
