Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Hinton,
annote = {The only thing ever to actually explain RMSProp.},
author = {Hinton, Geoffrey and N, Srivastava and {K. Swersky}},
file = {:home/ruben/Documents/Mendeley/Hinton, N, K. Swersky/Unknown/Lecture 6 Overview of mini-batch gradient descent - 2012 - Hinton, N, K. Swersky.pdf:pdf},
title = {{Lecture 6: Overview of mini-batch gradient descent}},
year = {2012}
}
@article{Lin1992,
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
annote = {Linked to by "Experience Replay for Real-Time Reinforcement Learning Control" as the paper that introduces Experience Replay.},
author = {Lin, Long J.},
doi = {10.1007/BF00992699},
file = {:home/ruben/Documents/Mendeley/Lin/Machine Learning/Self-improving reactive agents based on reinforcement learning, planning and teaching - 1992 - Lin.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,planning,teaching},
number = {3-4},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
volume = {8},
year = {1992}
}
@article{Mnih2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.01783v2},
author = {Mnih, Volodymyr and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray and Deepmind, Google},
eprint = {arXiv:1602.01783v2},
file = {:home/ruben/Documents/Mendeley/Mnih et al/Unknown/Asynchronous Methods for Deep Reinforcement Learning - 2016 - Mnih et al.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
volume = {48},
year = {2016}
}
@phdthesis{Vaerenbergh2011,
author = {Vaerenbergh, Kevin Van},
file = {:home/ruben/Documents/Mendeley/Vaerenbergh/Unknown/Improving Clutch Engagement with Reinforcement Learning Toepassen van Reinforcement Learning ter Verbetering van het Schakelgedrag van T.pdf:pdf},
title = {{Improving Clutch Engagement with Reinforcement Learning Toepassen van Reinforcement Learning ter Verbetering van het Schakelgedrag van Transmissies}},
year = {2011}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
annote = {An alternative solutions to the gradient problems},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/ruben/Documents/Mendeley/Pascanu, Mikolov, Bengio/Proceedings of The 30th International Conference on Machine Learning/On the difficulty of training recurrent neural networks - 2012 - Pascanu, Mikolov, Bengio.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Machado2015,
author = {Machado, Marlos C and Hausknecht, Matthew and Bellemare, Marc G},
file = {:home/ruben/Documents/Mendeley/Machado, Hausknecht, Bellemare/Unknown/Arcade Learning Environment Technical Manual (v.0.5.1) - 2015 - Machado, Hausknecht, Bellemare.pdf:pdf},
pages = {1--20},
title = {{Arcade Learning Environment Technical Manual (v.0.5.1)}},
year = {2015}
}
@article{Keerthi1994,
abstract = {This paper gives a compact, self-contained tutorial survey of reinforcement learning, a tool that is increasingly finding application in the development of intelligent dynamic systems. Research on rein-forcement learning during the past decade has led to the development of a variety of useful algorithms. This paper surveys the literature and presents the algorithms in a cohesive framework.},
author = {Keerthi, S Sathiya and Ravindran, B},
file = {:home/ruben/Documents/Mendeley/Keerthi, Ravindran/Unknown/A tutorial survey of reinforcement learn- ing - 1994 - Keerthi, Ravindran.pdf:pdf},
keywords = {Reinforcement learning,dynamic programming,neural networks,optimal control},
number = {6},
pages = {851--889},
title = {{A tutorial survey of reinforcement learn- ing}},
volume = {19},
year = {1994}
}
@article{Glorot2011a,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
annote = {Linked to by 'Empirical Evaluation of Rectified Activations in Convolution Network': ". It is commonly believed that the superior performance of ReLU comes from the sparsity"

Linked from Wikipedia when looking up Rectifier.
It's an easily computable activation function.

- Especially good for deep networks, i.e. 3 hidden layers or more.
- Since it's neither symmetric or anti-symmetric, apparently need twice the amount of hidden layers to compensate.
- Can use sparsity to distribute the gradients better.
- Take mathematical advantage.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:home/ruben/Documents/Mendeley/Glorot, Bordes, Bengio/Aistats/Deep Sparse Rectifier Neural Networks - 2011 - Glorot, Bordes, Bengio.pdf:pdf},
issn = {15324435},
journal = {AISTATS '11: Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
pages = {315--323},
title = {{Deep sparse rectifier neural networks}},
volume = {15},
year = {2011}
}
@article{Ryoo2015,
abstract = {In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient lo-cal motion observed in videos. We describe a representa-tion framework based on time series pooling, which is de-signed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how de-scriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.},
archivePrefix = {arXiv},
arxivId = {1412.6505},
author = {Ryoo, M S and Rothrock, Brandon and Matthies, Larry},
eprint = {1412.6505},
file = {:home/ruben/Documents/Mendeley/Ryoo, Rothrock, Matthies/Unknown/Pooled Motion Features for First-Person Videos - supplementary material - - 2015 - Ryoo, Rothrock, Matthies.pdf:pdf},
isbn = {9781467369640},
pages = {6964},
title = {{Pooled Motion Features for First-Person Videos - supplementary material -}},
year = {2015}
}
@article{Silver2015,
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Silver, David and Deepmind, Google},
eprint = {1507.04296},
file = {:home/ruben/Documents/Mendeley/Silver, Deepmind/Unknown/Deep Reinforcement Learning - 2015 - Silver, Deepmind.pdf:pdf},
title = {{Deep Reinforcement Learning}},
year = {2015}
}
@article{VanHasselt2015,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
eprint = {1509.06461},
file = {:home/ruben/Documents/Mendeley/van Hasselt, Guez, Silver/arXiv1509.06461 cs/Deep Reinforcement Learning with Double Q-learning - 2015 - van Hasselt, Guez, Silver.pdf:pdf},
journal = {arXiv:1509.06461 [cs]},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461$\backslash$nhttp://www.arxiv.org/pdf/1509.06461.pdf},
year = {2015}
}
@article{Sharma2015,
abstract = {We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.},
archivePrefix = {arXiv},
arxivId = {1511.04119},
author = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
eprint = {1511.04119},
file = {:home/ruben/Downloads/action-recognition-using-visual-attention-nips2015.pdf:pdf},
journal = {arXiv preprint},
pages = {1--11},
title = {{Action Recognition using Visual Attention}},
url = {http://arxiv.org/abs/1511.04119},
year = {2015}
}
@article{Maei2009,
abstract = {We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms effectiveness.},
annote = {Linked to by DeepMind's paper},
author = {Maei, H and Szepesvari, Csaba and Bhathnagar, S and Silver, D and Precup, D and Sutton, R},
file = {:home/ruben/Documents/Mendeley/Maei et al/Advances in Neural Information Processing Systems/Convergent temporal-difference learning with arbitrary smooth function approximation - 2009 - Maei et al.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
pages = {1--9},
title = {{Convergent temporal-difference learning with arbitrary smooth function approximation}},
year = {2009}
}
@techreport{Sutton,
author = {Sutton},
file = {:home/ruben/Downloads/SSR-98TR.pdf:pdf},
title = {{SSR-98TR.pdf}}
}
@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
annote = {Investigates capabilities of neural networks},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
file = {:home/ruben/Documents/Mendeley/Hornik/Neural Networks/Approximation capabilities of multilayer feedforward networks - 1991 - Hornik.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
number = {2},
pages = {251--257},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
annote = {NVidia uses this in their CuDNN implementation.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/ruben/Documents/Mendeley/Ioffe, Szegedy/arXiv/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - 2015 - Ioffe, Szegedy.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Cottreil1991,
author = {Cottreil, G W},
file = {:home/ruben/Documents/Mendeley/Cottreil/Proceedings of the 1990 Summer School on Connectionist Models/Extracting Features From Faces Using Compesssion Networks Face, Identity, Emotion, and Gender Recognition Using Holons - 1991 - Cottreil.pdf:pdf},
journal = {Proceedings of the 1990 Summer School on Connectionist Models},
pages = {328--338},
title = {{Extracting Features From Faces Using Compesssion Networks: Face, Identity, Emotion, and Gender Recognition Using Holons}},
year = {1991}
}
@article{Wang2016,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning inspired by advantage learning. Our dueling architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.},
annote = {Linked to by "Prioritized Experience Replay"},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and de Freitas, Nando and Lanctot, Marc},
eprint = {1511.06581},
file = {:home/ruben/Documents/Mendeley/Wang, de Freitas, Lanctot/arXiv/Dueling Network Architectures for Deep Reinforcement Learning - 2016 - Wang, de Freitas, Lanctot.pdf:pdf},
journal = {arXiv},
number = {9},
pages = {1--16},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.06581},
year = {2016}
}
@inproceedings{Ciresan2011,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
annote = {Linked to be wiki cnn article},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J{\"{u}}rgen},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
eprint = {arXiv:1011.1669v3},
file = {:home/ruben/Documents/Mendeley/Cireşan et al/IJCAI International Joint Conference on Artificial Intelligence/Flexible, high performance convolutional neural networks for image classification - 2011 - Cireşan et al.pdf:pdf},
isbn = {9781577355120},
issn = {10450823},
keywords = {Machine Learning},
pages = {1237--1242},
pmid = {21310177},
title = {{Flexible, high performance convolutional neural networks for image classification}},
year = {2011}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555},
file = {:home/ruben/Documents/Mendeley/Chung et al/Unknown/Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling - 2014 - Chung et al.pdf:pdf},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
annote = {Cognitron, predecessor of CNN},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:home/ruben/Documents/Mendeley/Fukushima/Biological Cybernetics/Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position - 1980 - Fuku.pdf:pdf},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
annote = {max pooling not always necessary, just filters with larger strides work just as well

linked from wiki page on cnn},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:home/ruben/Documents/Mendeley/Springenberg et al/Iclr/Striving for Simplicity The All Convolutional Net - 2015 - Springenberg et al.pdf:pdf},
isbn = {9781600066634},
journal = {Iclr},
pages = {1--14},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2015}
}
@article{Jarrett2009,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({\&}gt; 65{\%}), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53{\%}).},
annote = {Referred by DeepMind's paper as a motivation for their convolutional architecture.},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
doi = {10.1109/ICCV.2009.5459469},
file = {:home/ruben/Documents/Mendeley/Jarrett et al/Proceedings of the IEEE International Conference on Computer Vision/What is the best multi-stage architecture for object recognition - 2009 - Jarrett et al.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2146--2153},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@article{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
doi = {10.1007/11564096_32},
file = {:home/ruben/Documents/Mendeley/Riedmiller/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method - 2005 - Riedmiller.pdf:pdf},
isbn = {3540292438},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {317--328},
title = {{Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method}},
volume = {3720 LNAI},
year = {2005}
}
@inproceedings{kohavi1995study,
author = {Kohavi, Ron and Others},
booktitle = {Ijcai},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
volume = {14},
year = {1995}
}
@article{Bakker2001,
author = {Bakker, Bram},
file = {:home/ruben/Documents/Mendeley/Bakker/Nips 2001/Reinforcement Learning with Long Short-Term Memory - 2001 - Bakker.pdf:pdf},
journal = {Nips 2001},
title = {{Reinforcement Learning with Long Short-Term Memory}},
year = {2001}
}
@article{rumelhart1988learning,
annote = {Backpropagation introduced},
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
journal = {Cognitive modeling},
number = {3},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {5},
year = {1988}
}
@article{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:home/ruben/Documents/Mendeley/Szepesv{\'{a}}ri/Morgan and Claypool Publishers/Reinforcement learning algorithms for MDPs - 2010 - Szepesv{\'{a}}ri.pdf:pdf},
isbn = {9781608454921},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
volume = {4},
year = {2010}
}
@article{Moore1993,
abstract = {We present a new algorithm, Prioritized Sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Differencing and Qlearning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have difficulty.  1 Introduction  This paper introduces a memory-based technique, prioritized sweeping, which can be used both for Markov prediction and reinforcement learning. Current, model-free, learning algorithms perform well relative to real time. Classical meth...},
author = {Moore, Andrew W. and Atkeson, Christopher G.},
doi = {10.1.1.28.7241},
file = {:home/ruben/Documents/Mendeley/Moore, Atkeson/Machine Learning/Prioritized Sweeping Reinforcement Learning with Less Data and Less Real Time - 1993 - Moore, Atkeson.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
pages = {103--130},
title = {{Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.7241},
year = {1993}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
annote = {adagrad},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/ruben/Documents/Mendeley/Duchi, Hazan, Singer/Journal of Machine Learning Research/Adaptive Subgradient Methods for Online Learning and Stochastic Optimization - 2011 - Duchi, Hazan, Singer.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Silver2014,
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:home/ruben/Documents/Mendeley/Silver et al/Proc. of the 31st International Conference on Machine Learning/Deterministic Policy Gradient Algorithms - 2014 - Silver et al.pdf:pdf},
journal = {Proc. of the 31st International Conference on Machine Learning},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://jmlr.org/proceedings/papers/v32/silver14.html},
year = {2014}
}
@article{Bellemare2012,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
file = {:home/ruben/Documents/Mendeley/Bellemare et al/Unknown/The Arcade Learning Environment An Evaluation Platform for General Agents - 2012 - Bellemare et al.pdf:pdf},
pages = {253--279},
title = {{The Arcade Learning Environment: An Evaluation Platform for General Agents}},
url = {http://arxiv.org/abs/1207.4708 http://dx.doi.org/10.1613/jair.3912},
volume = {47},
year = {2012}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
annote = {Introduces LSTM},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, J},
file = {:home/ruben/Documents/Mendeley/Hochreiter, Urgen Schmidhuber/Neural Computation/Long Short-Term Memory - 1997 - Hochreiter, Urgen Schmidhuber.pdf:pdf},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit http://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Veness,
annote = {Peter gelinkt: over het BRUTE algoritme. Deterministische actie sequenties.},
author = {Veness, Joel and Bellemare, Marc G},
file = {:home/ruben/Documents/Mendeley/Veness, Bellemare/Unknown/AAAI-15 Workshop W18 - Unknown - Veness, Bellemare.pdf:pdf},
title = {{AAAI-15 Workshop W18}}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
annote = {The paper.

Good takeaway about RL in general: The data evolves as you go, it's not a fixed like ML, where the underlying distribution doesn't change.

RMSProp with minibatches of size 32. Get a paper on this.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/ruben/Documents/Mendeley/Mnih et al/NIPS/Playing Atari with Deep Reinforcement Learning - 2013 - Mnih et al.pdf:pdf},
issn = {0028-0836},
journal = {NIPS},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Muller2011,
abstract = {Nurses and physicians on a stroke unit constantly face pressure and emotional stress. Physiological sensors can create awareness of one's own stress and persuade medical staff to reflect on their own behavior and coping strategies. In this study, eight nurses and physicians of a stroke unit were equipped with a wearable electrocardiography (ECG) and acceleration sensor during their everyday work in order to (a) make them aware of stress and (b) support the re-calling of experiences to identify stressors. In an interview one week later, the participants were asked to recollect stress related events through the examination of the sensor data. Although high activity levels diminished the expressiveness of the data, physicians and nurses could recall stressful events and were interested in their physiological signals. However, existing coping strategies turned out as barriers to the adoption of new tools. Future persuasive applications should focus on integration with existing coping strategies to scaffold the reflection process. {\textcopyright} 2011 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03607v1},
author = {M{\"{u}}ller, Lars and Rivera-Pelayo, Ver{\'{o}}nica and Kunzmann, Christine and Schmidt, Andreas},
doi = {10.1007/978-3-642-25446-8},
eprint = {arXiv:1506.03607v1},
file = {:home/ruben/Documents/Mendeley/M{\"{u}}ller et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Sequential Deep Learning for Human Action Recognition - 2011 - M{\"{u}}ller et al.pdf:pdf},
isbn = {978-3-642-25445-1},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Reflective learning,healthcare,physiological sensor,user study},
number = {September 2015},
pages = {93--103},
pmid = {19556198},
title = {{Sequential Deep Learning for Human Action Recognition}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-81855225366{\&}partnerID=tZOtx3y1},
volume = {7065},
year = {2011}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
file = {:home/ruben/Documents/Mendeley/Graves/arXiv preprint arXiv1308.0850/Generating sequences with recurrent neural networks - 2013 - Graves.pdf:pdf},
isbn = {2000201075},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{woodworth1938experimental,
author = {Woodworth, Robert S and Schlosberg, Harold},
journal = {New York},
pages = {327--328},
title = {{Experimental Psychology Holt}},
year = {1938}
}
@article{Schaul2015,
author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
file = {:home/ruben/Documents/Mendeley/Schaul et al/Proceedings of The 32nd International Conference on Machine Learning/Universal Value Function Approximators - 2015 - Schaul et al.pdf:pdf},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {1312--1320},
title = {{Universal Value Function Approximators}},
url = {http://jmlr.org/proceedings/papers/v37/schaul15.html},
year = {2015}
}
@article{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1{\%}, [22]) on this visual recognition challenge.},
annote = {Introduces PRELU},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {:home/ruben/Documents/Mendeley/He et al/arXiv preprint/Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification - 2015 - He et al.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {arXiv preprint},
pages = {1--11},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Kaelbling1996,
abstract = {This paper surveys the of reinforcement learning from a computer-science per-spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the and a broad selection of current w ork are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but diiers considerably in the details and in the use of the word The paper discusses central issues of reinforcement learning, including trading oo exploration and exploitation, establishing the foundations of the via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
annote = {THE RL survey},
author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W and Hall, Smith},
file = {:home/ruben/Documents/Mendeley/Kaelbling et al/Journal of Artiicial Intelligence Research Submitted/Reinforcement Learning A Survey - 1996 - Kaelbling et al.pdf:pdf},
journal = {Journal of Artiicial Intelligence Research Submitted},
number = {9995},
pages = {237--285},
title = {{Reinforcement Learning: A Survey}},
volume = {4},
year = {1996}
}
@article{thorndike1911,
author = {Thorndike, Edward Lee},
title = {{Animal Intelligence}},
year = {1911}
}
@article{Kober2013,
author = {Kober, J. and Bagnell, J. A. and Peters, J.},
doi = {10.1177/0278364913495721},
file = {:home/ruben/Documents/Mendeley/Kober, Bagnell, Peters/The International Journal of Robotics Research/Reinforcement learning in robotics A survey - 2013 - Kober, Bagnell, Peters.pdf:pdf},
isbn = {9783642276446},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {learning control,reinforcement learning,robot,survey},
number = {11},
pages = {1238--1274},
title = {{Reinforcement learning in robotics: A survey}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364913495721},
volume = {32},
year = {2013}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
annote = {LeNet-5},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/ruben/Documents/Mendeley/LeCun et al/Proceedings of the IEEE/Gradient-based learning applied to document recognition - 1998 - LeCun et al.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Michie1968,
abstract = {BOXES is the name of a computer program. We shall also use the word boxes to refer to a particular approach to decision-taking under uncertainty which has been used as the basis of a number of computer programs. Fig. 1 shows a photograph of an assemblage of actual boxes—matchboxes to be exact. Although the construction of this Matchbox Educable Noughts and Crosses Engine (Michie 1961, 1963) was undertaken as a 'fun project', there was present a more serious intention to demonstrate the principle that it may be easier to learn to play many easy games than one difficult one. Consequently it may be advantageous to decompose a game into a number of mutually independent sub-games even if much relevant information is put out of reach in the process. The principle is related to the method of subgoals in problem-solving (see Newell et al. 1960) but differs in one fundamental respect: subgoals are linked in series, while sub-games are played in parallel, in a sense which will become apparent. DECOMPOSITION INTO SUB-GAMES},
author = {Michie, D and Chambers, R A},
file = {:home/ruben/Documents/Mendeley/Michie, Chambers/Machine Intelligence 2/BOXES An Experiment in Adaptive Control - 1968 - Michie, Chambers.pdf:pdf},
journal = {Machine Intelligence 2},
pages = {137--152},
title = {{BOXES: An Experiment in Adaptive Control}},
year = {1968}
}
@article{Y.2015dl,
abstract = {NATURE METHODS | VOL.13 NO.1 | JANUARY 2016 | 35 METHODS TO WATCH | SPECIAL FEATURE and high computational costs are being tackled. Researchers in academic settings as well as in startup companies such as Deep Genomics, launched July 22, 2015, by some of the authors of DeepBind, will increasingly apply deep learning to genome analysis and precision medicine. The goal is to predict the effect of genetic variants— both naturally occurring and introduced by genome editing—on a cell's regulatory landscape and how this in turn affects dis-ease development. Nicole Rusk ❯❯Deep learning New computational tools learn complex motifs from large sequence data sets. A powerful form of machine learning that enables computers to solve perceptual problems such as image and speech rec-ognition is increasingly making an entry into the biological sciences. These deep-learning methods, such as deep artificial neural networks, use multiple processing layers to discover patterns and structure in very large data sets. Each layer learns a concept from the data that subsequent lay-ers build on; the higher the level, the more abstract the concepts that are learned. Deep learning does not depend on prior data processing and automatically extracts features. To use a simple example, a deep neural network tasked with interpreting shapes would learn to recognize simple edges in the first layer and then add recog-nition of the more complex shapes com-posed of those edges in subsequent lay-ers. There is no hard and fast rule for how many layers are needed to constitute deep learning, but most experts agree that more than two are required. Recent examples show the power of deep learning to derive regulatory fea-tures in genomes from DNA sequence alone: DeepSEA (Nat. Methods 12, 931– 934, 2015) uses genomic sequence as input, trains on chromatin profiles from large consortia such as ENCODE and the Epigenomics Roadmap, and predicts the effect of single-nucleotide variants on reg-ulatory regions such as DNase hypersen-sitive sites, transcription factor–binding sites and histone marks. Basset (bioRxiv, doi:10.1101/028399, 2015) uses similar deep neural networks to predict the effect of single-nucleotide polymorphisms on chromatin accessibility. DeepBind (Nat. Biotechnol. 33, 831–838, 2015) finds protein-binding sites on RNA and DNA and predicts the effects of mutations. Deep learning will be invaluable in the context of big data, as it extracts high-level information from very large volumes of data. As it gains traction in genome analy-sis, initial challenges such as overfitting due to rare dependencies in the training data},
annote = {I've got the presentation version of this

Vanishing gradient makes RNNs a bitch to train},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Y. and Bengio, Y. and Hinton, G.},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:home/ruben/Documents/Mendeley/Lecun, Bengio, Hinton/Nature/Deep learning - 2015 - Lecun, Bengio, Hinton(2).pdf:pdf},
isbn = {9780521835688},
issn = {1548-7091},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539$\backslash$nhttp://www.nature.com/doifinder/10.1038/nmeth.3707},
volume = {521},
year = {2015}
}
@article{Hausknecht2014,
abstract = {This article addresses the challenge of learning to play many dierent video games with little domain- specic knowledge. Specically, it introduces a neuro-evolution approach to general Atari 2600 game playing. Four neuro-evolution algorithms were paired with three dierent state representations and evaluated on a set of 61 Atari games. The neuro-evolution agents represent dierent points along the spectrum of algorithmic sophistication - including weight evolution on topologically xed neural net- works (Conventional Neuro-evolution), Covariance Matrix Adaptation Evolution Strategy (CMA-ES), evolution of network topology and weights (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e. HyperNEAT) allow scaling to higher-dimensional representations (i.e. the raw game screen). Previous approaches based on temporal- dierence learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuro-evolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuro-evolution is a promising approach to general video game playing.},
author = {Hausknecht, Matthew and Lehman, Joel and Miikkulainen, Risto and Stone, Peter},
doi = {10.1109/TCIAIG.2013.2294713},
file = {:home/ruben/Documents/Mendeley/Hausknecht et al/IEEE Transactions on Computational Intelligence and AI in Games/A neuroevolution approach to general atari game playing - 2014 - Hausknecht et al.pdf:pdf},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Algorithms,artificial neural networks,evolutionary computation,genetic algorithms,neural networks},
number = {4},
pages = {355--366},
title = {{A neuroevolution approach to general atari game playing}},
volume = {6},
year = {2014}
}
@article{Ji2013,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
annote = {Linked to by Two-Stream Convolutional Networks for Action Recognition in Videos

Similar to Large-Scale Video Classification with Convolutional Neural Networks (appeared in same context)},
author = {Ji, Shuiwang and Yang, Ming and Yu, Kai and Xu, Wei},
doi = {10.1109/TPAMI.2012.59},
file = {:home/ruben/Documents/Mendeley/Ji et al/IEEE transactions on pattern analysis and machine intelligence/3D convolutional neural networks for human action recognition - 2013 - Ji et al.pdf:pdf},
isbn = {9781605589077},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Decision Support Techniques,Image Interpretation,Imaging,Movement,Movement: physiology,Neural Networks (Computer),Pattern Recognition,Subtraction Technique,Three-Dimensional,Three-Dimensional: methods},
number = {1},
pages = {221--31},
pmid = {22392705},
title = {{3D convolutional neural networks for human action recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6165309$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/22392705},
volume = {35},
year = {2013}
}
@book{friedman2001elements,
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
publisher = {Springer series in statistics Springer, Berlin},
title = {{The elements of statistical learning}},
volume = {1},
year = {2001}
}
@article{Widrow1990,
author = {Widrow, B and Lehr, M a},
file = {:home/ruben/Documents/Mendeley/Widrow, Lehr/Proceedings of the Ieee/30 Years of Adaptive Neural Networks - Perception, Madaline, and Backpropagation - 1990 - Widrow, Lehr.pdf:pdf},
journal = {Proceedings of the Ieee},
number = {9},
pages = {1415--1442},
title = {{30 Years of Adaptive Neural Networks - Perception, Madaline, and Backpropagation}},
url = {{\textless}Go to ISI{\textgreater}://A1990EC03500002},
volume = {78},
year = {1990}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
annote = {Presentation to equinamed paper. Introduces vanishing gradient or so I'm told. About RNN},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Y and Simard, P and Frasconi, P},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/ruben/Documents/Mendeley/Bengio, Simard, Frasconi/IEEE Transactions on Neural Networks/Learning Long Term Dependencies with Gradient Descent is Difficult - 1994 - Bengio, Simard, Frasconi.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new ran- domized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart.},
annote = {Good overview on relu family,
don't include in thesis. Contains good refs.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:home/ruben/Documents/Mendeley/Xu et al/ICML Deep Learning Workshop/Empirical Evaluation of Rectified Activations in Convolution Network - 2015 - Xu et al.pdf:pdf},
journal = {ICML Deep Learning Workshop},
pages = {1--5},
title = {{Empirical Evaluation of Rectified Activations in Convolution Network}},
year = {2015}
}
@article{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these con-trollers have limited memory and rely on being able to perceive the complete game screen at each deci-sion point. To address these shortcomings, this arti-cle investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recur-rent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with in-crementally more complete observations, DRQN's per-formance scales as a function of observability. Con-versely, when trained with full observations and eval-uated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learn-ing to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
annote = {There are btw two ways of doing updates.
They stick to one in this paper but explain both
and explain how they both work fairly well.

Extra for LSTM: gradients clipped to 10 for stability

Rectifier layer after LSTM reduced performance

Note: This paper contains a very good discussion on MDP/POMDP and includes nice visual reps of convolutional layers and Pong.

Exactly my first attempt at such an architecture: first post-convolutional layer is replaced with an LSTM layer.

DQRN only replicates performance and performs better on incomplete observations. Otherwise, it doesn't improve (according to Abstract).

They quote the DeepMind paper to not be able to find any games that are still only partially observable given 4 frames. However, I have a paper stating that some games need 18 frames. What's up with this?

Very nice approach to determining whether the obscured-trained DRQN can work well in non-obscured situations (to test for generalization). They do this by comparing activations between situations of p=.5 and p=1 (p=chance of obscuring). They differ only slightly, as opposed to the normal DQN.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
file = {:home/ruben/Documents/Mendeley/Hausknecht, Stone/arXiv preprint arXiv1507.06527/Deep Recurrent Q-Learning for Partially Observable MDPs - 2015 - Hausknecht, Stone.pdf:pdf},
journal = {arXiv preprint arXiv:1507.06527},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
year = {2015}
}
@article{Lin1993,
author = {Lin, Long-ji},
file = {:home/ruben/Documents/Mendeley/Lin/Report, CMU/Reinforcement Learning for Robots Using Neural Networks - 1993 - Lin.pdf:pdf},
journal = {Report, CMU},
pages = {1--155},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@article{lovejoy1991survey,
author = {Lovejoy, William S},
journal = {Annals of Operations Research},
number = {1},
pages = {47--65},
publisher = {Springer},
title = {{A survey of algorithmic methods for partially observed Markov decision processes}},
volume = {28},
year = {1991}
}
@article{Bellemare2015,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a chal- lenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and compar- ing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available. 1.},
annote = {Peter: "in deze paper vermelden ze dat het gedrag van de paddle afhankelijk is van de laatste 18 actie"},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.4708v2},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
doi = {10.1613/jair.3912},
eprint = {arXiv:1207.4708v2},
file = {:home/ruben/Documents/Mendeley/Bellemare et al/IJCAI International Joint Conference on Artificial Intelligence/The arcade learning environment An evaluation platform for general agents - 2015 - Bellemare et al.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4148--4152},
title = {{The arcade learning environment: An evaluation platform for general agents}},
volume = {2015-Janua},
year = {2015}
}
@article{Williams1995,
abstract = {Recurrent connectionist networks are important because they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks that which have an independent temporal component not easily handled through the use of simple tapped delay lines. Some examples are tasks involving recognition or generation of sequential patterns and sensorimotor control. This report examines a number of learning procedures for adjusting the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples. The procedures are all based on the computation of the gradient of performance error with respect to network weights, and a number of strategies for computing the necessary gradient information are described. Included here are approaches which are familiar and have been first described elsewhere, along with several novel approaches. One particular purpose of this report is to provide uniform and detailed descriptions and derivations of the various techniques in order to emphasize how they relate to one another. Another important contribution of this report is a detailed analysis of the computational requirements of the various approaches discussed.},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1080/02673039508720837},
file = {:home/ruben/Documents/Mendeley/Williams, Zipser/Back-propagation Theory, architectures and Applications/Gradient-based learning algorithms for recurrent networks and their computational complexity - 1995 - Williams, Zipser.pdf:pdf},
isbn = {0-8058-1259-8},
journal = {Back-propagation: Theory, architectures and Applications},
pages = {1--45},
title = {{Gradient-based learning algorithms for recurrent networks and their computational complexity}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=kvbx6dVv2uEC{\&}oi=fnd{\&}pg=PA433{\&}dq=Gradient-Based+Learning+Algorithms+for+Recurrent+Networks+and+Their+Computational+Complexity+1+Introduction{\&}ots=MAFeylo232{\&}sig=-L3olgtrd3Do{\_}LZZ{\_}DQcwigyI},
year = {1995}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:home/ruben/Documents/Mendeley/Hornik, Stinchcombe, White/Neural Networks/Multilayer feedforward networks are universal approximators - 1989 - Hornik, Stinchcombe, White.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Stadie2015a,
abstract = {Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. To achieve more efficient exploration, we develop a method for assigning exploration bonuses based on a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. We demonstrate our approach on the task of learning to play Atari games from raw pixel inputs. In this domain, our method offers substantial improvements in exploration efficiency when compared with the standard epsilon greedy approach. As a result of our improved exploration strategy, we are able to achieve state-of-the-art results on several games that pose a major challenge for prior methods.},
archivePrefix = {arXiv},
arxivId = {1507.00814},
author = {Stadie, Bradly C. and Levine, Sergey and Abbeel, Pieter},
eprint = {1507.00814},
file = {:home/ruben/Documents/Mendeley/Stadie, Levine, Abbeel/arXiv/Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models - 2015 - Stadie, Levine, Abbeel.pdf:pdf},
journal = {arXiv},
pages = {1--11},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}},
url = {http://arxiv.org/abs/1507.00814},
year = {2015}
}
@article{Silver2016,
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/ruben/Documents/Mendeley/Silver et al/Nature/Mastering the game of Go with deep neural networks and tree search - 2016 - Silver et al.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Karpathy2014,
abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new...},
annote = {Has three different ways of tackling CNNs with the time dimension. Slow merge is most interesting},
author = {Karpathy, Andrej and Toderici, George and Shetty, Sachin and Leung, Tommy and Sukthankar, Rahul and Fei-Fei, Li},
doi = {10.1109/CVPR.2014.223},
file = {:home/ruben/Documents/Mendeley/Karpathy et al/2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)/Large-Scale Video Classification with Convolutional Neural Networks - 2014 - Karpathy et al.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {978-1-4799-5118-5},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1725--1732},
title = {{Large-Scale Video Classification with Convolutional Neural Networks}},
url = {http://cs.stanford.edu/people/karpathy/deepvideo http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909619 papers3://publication/doi/10.1109/CVPR.2014.223},
year = {2014}
}
@article{Fallis2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Good section on sparse coarse-coded function approximators},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/ruben/Downloads/tech-report.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Experiments with Reinforcement Learning}},
volume = {53},
year = {2013}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/ruben/Documents/Mendeley/Mnih et al/Nature/Human-level control through deep reinforcement learning - 2015 - Mnih et al.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Sindhwani2005,
annote = {I don't understand everything about kernel functions.},
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
file = {:home/ruben/Documents/Mendeley/Sindhwani, Niyogi, Belkin/Proceedings of ICML Workshop on Learning with Multiple Views/A co-regularization approach to semi-supervised learning with multiple views - 2005 - Sindhwani, Niyogi, Belkin.pdf:pdf},
journal = {Proceedings of ICML Workshop on Learning with Multiple Views},
pages = {74--79},
title = {{A co-regularization approach to semi-supervised learning with multiple views}},
year = {2005}
}
@article{Bruin2015,
abstract = {Recent years have seen a growing interest in the use of deep neural networks as function approximators in reinforcement learning. This paper investigates the po- tential of the DeepDeterministic PolicyGradientmethod for a robot control prob- lem both in simulation and in a real setup. The importance of the size and com- position of the experience replay database is investigated and some requirements on the distribution over the state-action space of the experiences in the database are identified. Of particular interest is the importance of negative experiences that are not close to an optimal policy. It is shown how training with samples that are insufficiently spread over the state-action space can cause the method to fail, and how maintaining the distribution over the state-action space of the samples in the experience database can greatly benefit learning.},
annote = {Paper has a good theoretical background, maybe interesting for thesis.
Takeaway: sample database needs to be and stay representative, otherwise previously learned can still be forgotten. Also interesting: Keeping oldest and newest samples works better than just twice the amount of newest samples. 

Interesting note on future work: better policy to replace samples in the db. Links to 19 and 20.

"It is shown how training with samples that are insufficiently spread over the state-action space can cause the method to fail, and how maintaining the distribution over the state-action space of the samples in the experience database can greatly benefit learning.

"When this is combined with the fact that neural networks can easily forget about data that they are no longer trained on [16], it becomes apparent that when an experience database of limited size is used, the contents should be maintained"

"Additionally, even if a network has previously learned to do a task well, it can forget this knowledge completely when learning a new task, even when the new task is related to the old one"

"Regularization can help prevent neural networks from over-fitting to their training data. However, the regularization of neural networks used for control is currently not as well understood as the reg- ularization of neural networks in other domains [17]. It is therefore important to ensure the training data in the experience database is varied enough for the neural networks to properly generalize their knowledge to the whole state-action space. A simplemethod that helps with this issue is introduced in Section 4."},
author = {de Bruin, Tim and Kober, Jens and Tuyls, Karl and Babuska, Robert},
file = {:home/ruben/Documents/Mendeley/Bruin et al/Unknown/The importance of experience replay database composition in deep reinforcement learning - 2015 - Bruin et al.pdf:pdf},
pages = {1--9},
title = {{The importance of experience replay database composition in deep reinforcement learning}},
url = {rll.berkeley.edu/deeprlworkshop/papers/database{\_}composition.pdf},
year = {2015}
}
@article{turing1948intelligent,
author = {Turing, Alan M},
journal = {The Turing Test: Verbal Behavior as the Hallmark of Intelligence},
publisher = {MIT Press},
title = {{Intelligent machinery, a heretical theory}},
volume = {105},
year = {1948}
}
@article{Adam2012,
annote = {Their results are better than native Q-learning or Sarsa. Main advantage seems to be that you can better use your experience. It mentions that it's better than some batch learning method.

According to Peter this paper's technique is used by DeepMind. 
He also suggests tracking trajectories could be an interesting extension since recurrent architectures always work with trajectories.},
author = {Adam, Sander and Buşoniu, Lucian and Babu{\v{s}}ka, Robert},
doi = {10.1109/TSMCC.2011.2106494},
file = {:home/ruben/Documents/Mendeley/Adam, Buşoniu, Babu{\v{s}}ka/IEEE Transactions on Systems, Man and Cybernetics Part C Applications and Reviews/Experience replay for real-time reinforcement learning control - 2012 - Adam, Buşoniu, Babu{\v{s}}ka.pdf:pdf},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Experience replay (ER),Q-learning,SARSA,real-time control,reinforcement learning (RL),robotics},
number = {2},
pages = {201--212},
title = {{Experience replay for real-time reinforcement learning control}},
volume = {42},
year = {2012}
}
@article{Y.2015a,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Y. and Bengio, Y. and Hinton, G.},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/ruben/Documents/Mendeley/Lecun, Bengio, Hinton/Nature/Deep learning - 2015 - Lecun, Bengio, Hinton.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
volume = {521},
year = {2015}
}
@article{Zhu2007,
abstract = {We review the literature on semi-supervised learning, which is an area in machine learning and more generally, artificial intelligence. There has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data, i.e. semi-supervised learning. This document originates as a chapter in the authors doctoral thesis (Zhu, 2005). However the author will update the online version regularly to incorporate the latest development in the field.},
author = {Zhu, Xiaojin},
doi = {10.1.1.146.2352},
file = {:home/ruben/Documents/Mendeley/Zhu/Sciences-New York/Semi-Supervised Learning Literature Survey - 2007 - Zhu.pdf:pdf},
isbn = {1530},
journal = {Sciences-New York},
keywords = {learning},
pages = {1--59},
pmid = {22175947},
title = {{Semi-Supervised Learning Literature Survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.2352{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{mccallum1995instance,
author = {McCallum, R Andrew},
booktitle = {ICML},
pages = {387--395},
title = {{Instance-based utile distinctions for reinforcement learning with hidden state}},
year = {1995}
}
@article{Lin1992a,
abstract = {Reinforcement learning is a type of unsupervised learning for sequential decision making. Qlearning is probably the best-understood reinforcement learning algorithm. In Q-learning, the agent learns a mapping from states and actions to their utilities. An important assumption of Q-learning is the Markovian environment assumption, meaning that any information needed to determine the optimal actions is reflected in the agent's state representation. Consider an agent whose state representation is based solely on its immediate perceptual sensations. When its sensors are not able to make essential distinctions among world states, the Markov assumption is violated, causing a problem called perceptual aliasing. For example, when facing a closed box, an agent based on its current visual sensation cannot act optimally if the optimal action depends on the contents of the box. There are two basic approaches to addressing this problem-- using more sensors or using history to figure out the current world state. This paper studies three connectionist approaches which learn to use history to handle perceptual aliasing: the window-Q, recurrent-Q and recurrent-model architectures. Empirical study of these architectures is presented. Their relative strengths and weaknesses are also discussed.},
author = {Lin, L.J. and Mitchell, T.M.},
file = {:home/ruben/Documents/Mendeley/Lin, Mitchell/Artificial Intelligence/Memory approaches to reinforcement learning in non-Markovian domains - 1992 - Lin, Mitchell.pdf:pdf},
journal = {Artificial Intelligence},
keywords = {Markov/non-Markov decision task,action model,recurrent neural network,reinforcement learning,time-delay neural network},
number = {7597},
pages = {28},
title = {{Memory approaches to reinforcement learning in non-Markovian domains}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.319},
volume = {8},
year = {1992}
}
@article{watkins1992q,
author = {Watkins, Christopher J C H and Dayan, Peter},
journal = {Machine learning},
number = {3-4},
pages = {279--292},
publisher = {Springer},
title = {{Q-learning}},
volume = {8},
year = {1992}
}
@article{szepesvari2010reinforcement,
annote = {Book referred to as math text by the sutton book.},
author = {Szepesv{\'{a}}ri, Csaba},
file = {:home/ruben/Documents/Mendeley/Szepesv{\'{a}}ri/Morgan and Claypool Publishers/Reinforcement learning algorithms for MDPs - 2010 - Szepesv{\'{a}}ri.pdf:pdf},
journal = {Morgan and Claypool Publishers},
publisher = {Citeseer},
title = {{Reinforcement learning algorithms for MDPs}},
year = {2010}
}
@article{Sutton1998a,
abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
file = {:home/ruben/Documents/Mendeley/Sutton, Barto/IEEE Transactions on Neural Networks/Reinforcement Learning An Introduction - 1998 - Sutton, Barto.pdf:pdf},
isbn = {0262193981},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {intro to dynamic programming,ro to reinforcement learning},
number = {5},
pages = {1054--1054},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction}},
url = {http://dl.acm.org/citation.cfm?id=551283$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=712192},
volume = {9},
year = {1998}
}
@book{pavlov1927conditional,
author = {Pavlov, Ivan Petrovich},
publisher = {H. Milford},
title = {{Conditional reflexes: An investigation of the physiological activity of the cerebral cortex}},
year = {1927}
}
@misc{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1989.1.4.541},
file = {:home/ruben/Documents/Mendeley/LeCun et al/Neural Computation/Backpropagation Applied to Handwritten Zip Code Recognition - 1989 - LeCun et al.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
number = {4},
pages = {541--551},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
volume = {1},
year = {1989}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
annote = {Linked from Wikipedia when looking up Rectifier.
It's an easily computable activation function.

- Especially good for deep networks, i.e. 3 hidden layers or more.
- Since it's neither symmetric or anti-symmetric, apparently need twice the amount of hidden layers to compensate.
- Can use sparsity to distribute the gradients better.
- Take mathematical advantage.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:home/ruben/Documents/Mendeley/Glorot, Bordes, Bengio/Aistats/Deep Sparse Rectifier Neural Networks - 2011 - Glorot, Bordes, Bengio.pdf:pdf},
issn = {15324435},
journal = {Aistats},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
volume = {15},
year = {2011}
}
@article{Simonyan2014,
abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to incorporate into the network design aspects of the best performing hand-crafted features. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it matches the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
annote = {Two separate streams: one spatial one temporal.

The temporal model is trained on explicitly calculated flow displacement fields: basically they calculate motion across the img, encode that as pixels of an image and voila. I don't see the direction using in these "displacement pixels", magnitude seems to play the only role.

Oh also multi-frame optical flow, so not just 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2199v1},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1406.2199v1},
file = {:home/ruben/Documents/Mendeley/Simonyan, Zisserman/arXiv preprint arXiv1406.2199/Two-Stream Convolutional Networks for Action Recognition in Videos - 2014 - Simonyan, Zisserman.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {arXiv preprint arXiv:1406.2199},
pages = {1--11},
pmid = {25246403},
title = {{Two-Stream Convolutional Networks for Action Recognition in Videos}},
url = {http://arxiv.org/abs/1406.2199},
year = {2014}
}
@article{Heess2015,
abstract = {Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time. We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels. We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.},
annote = {That Google paper using an Actor-Critic architecture

I see very well why ALE was not used: It's simply too Markov. All problems considered here lack serious state information that has to be inferred from the system's history. Perfect for Recurrent networks.},
archivePrefix = {arXiv},
arxivId = {1512.04455},
author = {Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David},
eprint = {1512.04455},
file = {:home/ruben/Documents/Mendeley/Heess et al/Unknown/Memory-based control with recurrent neural networks - 2015 - Heess et al.pdf:pdf},
pages = {1--11},
title = {{Memory-based control with recurrent neural networks}},
url = {http://arxiv.org/abs/1512.04455},
year = {2015}
}
@article{Narasimhan2015,
abstract = {In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against a baseline with a bag-of-words state representation. Our algorithm outperforms the baseline on quest completion by 54{\%} on a newly created world and by 14{\%} on a pre-existing fantasy game.},
archivePrefix = {arXiv},
arxivId = {1506.08941},
author = {Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
eprint = {1506.08941},
file = {:home/ruben/Documents/Mendeley/Narasimhan, Kulkarni, Barzilay/Emnlp/Language Understanding for Text-based Games Using Deep Reinforcement Learning - 2015 - Narasimhan, Kulkarni, Barzilay.pdf:pdf},
journal = {Emnlp},
pages = {10},
title = {{Language Understanding for Text-based Games Using Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1506.08941},
year = {2015}
}
@article{Zhang2011,
abstract = {This paper proposes a hybrid crop classifier for polarimetric synthetic aperture radar (SAR) images. The feature sets consisted of span image, the H/A/$\alpha$ decomposition, and the gray-level co-occurrence matrix (GLCM) based texture features. Then, the features were reduced by principle component analysis (PCA). Finally, a two-hidden-layer forward neural network (NN) was constructed and trained by adaptive chaotic particle swarm optimization (ACPSO). K-fold cross validation was employed to enhance generation. The experimental results on Flevoland sites demonstrate the superiority of ACPSO to back-propagation (BP), adaptive BP (ABP), momentum BP (MBP), Particle Swarm Optimization (PSO), and Resilient back-propagation (RPROP) methods. Moreover, the computation time for each pixel is only 1.08 × 10(-7) s.},
author = {Zhang, Yudong and Wu, Lenan},
doi = {10.3390/s110504721},
file = {:home/ruben/Documents/Mendeley/Zhang, Wu/Sensors/Crop classification by forward neural network with adaptive chaotic particle swarm optimization - 2011 - Zhang, Wu.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Artificial neural network,Particle swarm optimization,Principle component analysis,Synthetic aperture radar},
month = {may},
number = {5},
pages = {4721--4743},
pmid = {22163872},
publisher = {Molecular Diversity Preservation International},
title = {{Crop classification by forward neural network with adaptive chaotic particle swarm optimization}},
url = {http://www.mdpi.com/1424-8220/11/5/4721/},
volume = {11},
year = {2011}
}
@inproceedings{Cirean,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J{\"{u}}rgen},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
eprint = {arXiv:1011.1669v3},
file = {:home/ruben/Documents/Mendeley/Cireşan et al/IJCAI International Joint Conference on Artificial Intelligence/Flexible, high performance convolutional neural networks for image classification - 2011 - Cireşan et al.pdf:pdf},
isbn = {9781577355120},
issn = {10450823},
keywords = {Machine Learning},
pages = {1237--1242},
pmid = {21310177},
title = {{Flexible, high performance convolutional neural networks for image classification}},
year = {2011}
}
@phdthesis{ring1994continual,
author = {Ring, Mark Bishop},
school = {University of Texas at Austin},
title = {{CONTINUAL LEARNING IN REINFORCEMENT ENVIRONM ENTS}},
year = {1994}
}
@article{VanDerMaaten2008,
abstract = {KNAW Narcis. Back to search results. Publication - - (2008). Pagina-navigatie: Main. Title, - - . Published in, Journal of Machine Learning Research, Vol. 9, No. nov, p.2579-2605.},
author = {{Van Der Maaten}, L J P and Hinton, G E},
doi = {10.1007/s10479-011-0841-3},
file = {:home/ruben/Documents/Mendeley/Van Der Maaten, Hinton/Journal of Machine Learning Research/Visualizing high-dimensional data using t-sne - 2008 - Van Der Maaten, Hinton.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing high-dimensional data using t-sne}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=7911431479148734548related:VOiAgwMNy20J},
volume = {9},
year = {2008}
}
@article{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our al-gorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is com-petitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies " end-to-end " : directly from raw pixel in-puts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971v2},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {arXiv:1509.02971v2},
file = {:home/ruben/Documents/Mendeley/Lillicrap et al/Unknown/Continuous Control With Deep Reinforcement Learning - 2016 - Lillicrap et al.pdf:pdf},
title = {{Continuous Control With Deep Reinforcement Learning}},
year = {2016}
}
@phdthesis{watkins1989learning,
author = {Watkins, Christopher John Cornish Hellaby},
school = {University of Cambridge England},
title = {{Learning from delayed rewards}},
year = {1989}
}
@article{Schaul2016,
annote = {Nice background chapter on biological experience replay. Good links.
Good literature overview on DQN. 

They have the same worries about overfitting so they make it stochastic. Exactly what I wanted to achieve.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.05952v4},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David and Deepmind, Google},
eprint = {arXiv:1511.05952v4},
file = {:home/ruben/Documents/Mendeley/Schaul et al/Unknown/Prioritized Experience Replay - 2016 - Schaul et al.pdf:pdf},
pages = {1--21},
title = {{Prioritized Experience Replay}},
year = {2016}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
annote = {So this paper seems to be written before the 2011 paper about Rectifiers. Should check it out, because if this one introduces Rectifiers I'm not sure what the point is of the other one.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:home/ruben/Documents/Mendeley/Nair, Hinton/Proceedings of the 27th International Conference on Machine Learning/Rectified Linear Units Improve Restricted Boltzmann Machines - 2010 - Nair, Hinton.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Cassandra1994,
abstract = {In this paper, we describe the partially observable Markov decision process (pomdp) approach to optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The pomdp approach was originally developed in the operations research com-munity and provides a formal basis for planning prob-lems that have been of interest to the AI community. We found the existing algorithms for computing op-timal control strategies to be highly computationally ineecient and have developed a new algorithm that is empirically more eecient. We sketch this algorithm and present preliminary results on several small prob-lems that illustrate important properties of the pomdp approach.},
author = {Cassandra, Anthony R and Kaelbling, Leslie Pack and Littman, M L L B - CassandraETAL.AAAI1994},
file = {:home/ruben/Documents/Mendeley/Cassandra, Kaelbling, Littman/Aaai/Acting optimaly in partially observable stochastic domains - 1994 - Cassandra, Kaelbling, Littman.pdf:pdf},
journal = {Aaai},
pages = {1023--1023},
title = {{Acting optimaly in partially observable stochastic domains}},
year = {1994}
}
