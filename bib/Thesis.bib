Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Lin1993,
author = {Lin, Long-ji},
file = {:home/ruben/Documents/Mendeley/Lin/Report, CMU/Reinforcement Learning for Robots Using Neural Networks - 1993 - Lin.pdf:pdf},
journal = {Report, CMU},
pages = {1--155},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@article{Bellemare2015,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a chal- lenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and compar- ing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available. 1.},
annote = {Peter: "in deze paper vermelden ze dat het gedrag van de paddle afhankelijk is van de laatste 18 actie"},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.4708v2},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
doi = {10.1613/jair.3912},
eprint = {arXiv:1207.4708v2},
file = {:home/ruben/Documents/Mendeley/Bellemare et al/IJCAI International Joint Conference on Artificial Intelligence/The arcade learning environment An evaluation platform for general agents - 2015 - Bellemare et al.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4148--4152},
title = {{The arcade learning environment: An evaluation platform for general agents}},
volume = {2015-Janua},
year = {2015}
}
@article{He2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1{\%}, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852},
file = {:home/ruben/Documents/Mendeley/He et al/arXiv preprint/Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification - 2015 - He et al.pdf:pdf},
journal = {arXiv preprint},
pages = {1--11},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Heess2015,
abstract = {Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time. We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels. We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.},
annote = {That Google paper using an Actor-Critic architecture

I see very well why ALE was not used: It's simply too Markov. All problems considered here lack serious state information that has to be inferred from the system's history. Perfect for Recurrent networks.},
archivePrefix = {arXiv},
arxivId = {1512.04455},
author = {Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David},
eprint = {1512.04455},
file = {:home/ruben/Documents/Mendeley/Heess et al/Unknown/Memory-based control with recurrent neural networks - 2015 - Heess et al.pdf:pdf},
pages = {1--11},
title = {{Memory-based control with recurrent neural networks}},
url = {http://arxiv.org/abs/1512.04455},
year = {2015}
}
@article{Adam2012,
annote = {Their results are better than native Q-learning or Sarsa. Main advantage seems to be that you can better use your experience. It mentions that it's better than some batch learning method.

According to Peter this paper's technique is used by DeepMind. 
He also suggests tracking trajectories could be an interesting extension since recurrent architectures always work with trajectories.},
author = {Adam, Sander and Buşoniu, Lucian and Babu{\v{s}}ka, Robert},
doi = {10.1109/TSMCC.2011.2106494},
file = {:home/ruben/Documents/Mendeley/Adam, Buşoniu, Babu{\v{s}}ka/IEEE Transactions on Systems, Man and Cybernetics Part C Applications and Reviews/Experience replay for real-time reinforcement learning control - 2012 - Adam, Buşoniu, Babu{\v{s}}ka.pdf:pdf},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Experience replay (ER),Q-learning,SARSA,real-time control,reinforcement learning (RL),robotics},
number = {2},
pages = {201--212},
title = {{Experience replay for real-time reinforcement learning control}},
volume = {42},
year = {2012}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:home/ruben/Documents/Mendeley/Hornik, Stinchcombe, White/Neural Networks/Multilayer feedforward networks are universal approximators - 1989 - Hornik, Stinchcombe, White.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Stadie2015a,
abstract = {Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. To achieve more efficient exploration, we develop a method for assigning exploration bonuses based on a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. We demonstrate our approach on the task of learning to play Atari games from raw pixel inputs. In this domain, our method offers substantial improvements in exploration efficiency when compared with the standard epsilon greedy approach. As a result of our improved exploration strategy, we are able to achieve state-of-the-art results on several games that pose a major challenge for prior methods.},
archivePrefix = {arXiv},
arxivId = {1507.00814},
author = {Stadie, Bradly C. and Levine, Sergey and Abbeel, Pieter},
eprint = {1507.00814},
file = {:home/ruben/Documents/Mendeley/Stadie, Levine, Abbeel/arXiv/Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models - 2015 - Stadie, Levine, Abbeel.pdf:pdf},
journal = {arXiv},
pages = {1--11},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}},
url = {http://arxiv.org/abs/1507.00814},
year = {2015}
}
@article{Silver2016,
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/ruben/Documents/Mendeley/Silver et al/Nature/Mastering the game of Go with deep neural networks and tree search - 2016 - Silver et al.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Hinton,
annote = {The only thing ever to actually explain RMSProp.},
author = {Hinton, Geoffrey},
file = {:home/ruben/Documents/Mendeley/Hinton/Ppt/Overview of mini-batch gradient descent - Unknown - Hinton.pdf:pdf},
journal = {Ppt},
title = {{Overview of mini-batch gradient descent}}
}
@article{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our al-gorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is com-petitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies " end-to-end " : directly from raw pixel in-puts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02971v2},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {arXiv:1509.02971v2},
file = {:home/ruben/Documents/Mendeley/Lillicrap et al/Unknown/Continuous Control With Deep Reinforcement Learning - 2016 - Lillicrap et al.pdf:pdf},
title = {{Continuous Control With Deep Reinforcement Learning}},
year = {2016}
}
@article{rumelhart1988learning,
annote = {Backpropagation introduced},
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
journal = {Cognitive modeling},
number = {3},
pages = {1},
title = {{Learning representations by back-propagating errors}},
volume = {5},
year = {1988}
}
@article{Fallis2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Good section on sparse coarse-coded function approximators},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/ruben/Downloads/tech-report.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Experiments with Reinforcement Learning}},
volume = {53},
year = {2013}
}
@article{Kaelbling1996,
abstract = {This paper surveys the of reinforcement learning from a computer-science per-spective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the and a broad selection of current w ork are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but diiers considerably in the details and in the use of the word The paper discusses central issues of reinforcement learning, including trading oo exploration and exploitation, establishing the foundations of the via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
annote = {THE RL survey},
author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W and Hall, Smith},
file = {:home/ruben/Documents/Mendeley/Kaelbling et al/Journal of Artiicial Intelligence Research Submitted/Reinforcement Learning A Survey - 1996 - Kaelbling et al.pdf:pdf},
journal = {Journal of Artiicial Intelligence Research Submitted},
number = {9995},
pages = {237--285},
title = {{Reinforcement Learning: A Survey}},
volume = {4},
year = {1996}
}
@article{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1{\%}, [22]) on this visual recognition challenge.},
annote = {Introduces PRELU},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {:home/ruben/Documents/Mendeley/He et al/arXiv preprint/Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification - 2015 - He et al.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {arXiv preprint},
pages = {1--11},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
annote = {Linked from Wikipedia when looking up Rectifier.
It's an easily computable activation function.

- Especially good for deep networks, i.e. 3 hidden layers or more.
- Since it's neither symmetric or anti-symmetric, apparently need twice the amount of hidden layers to compensate.
- Can use sparsity to distribute the gradients better.
- Take mathematical advantage.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:home/ruben/Documents/Mendeley/Glorot, Bordes, Bengio/Aistats/Deep Sparse Rectifier Neural Networks - 2011 - Glorot, Bordes, Bengio.pdf:pdf},
issn = {15324435},
journal = {Aistats},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
volume = {15},
year = {2011}
}
@article{Sindhwani2005,
annote = {I don't understand everything about kernel functions.},
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
file = {:home/ruben/Documents/Mendeley/Sindhwani, Niyogi, Belkin/Proceedings of ICML Workshop on Learning with Multiple Views/A co-regularization approach to semi-supervised learning with multiple views - 2005 - Sindhwani, Niyogi, Belkin.pdf:pdf},
journal = {Proceedings of ICML Workshop on Learning with Multiple Views},
pages = {74--79},
title = {{A co-regularization approach to semi-supervised learning with multiple views}},
year = {2005}
}
@article{Zhu2007,
abstract = {We review the literature on semi-supervised learning, which is an area in machine learning and more generally, artificial intelligence. There has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data, i.e. semi-supervised learning. This document originates as a chapter in the authors doctoral thesis (Zhu, 2005). However the author will update the online version regularly to incorporate the latest development in the field.},
author = {Zhu, Xiaojin},
doi = {10.1.1.146.2352},
file = {:home/ruben/Documents/Mendeley/Zhu/Sciences-New York/Semi-Supervised Learning Literature Survey - 2007 - Zhu.pdf:pdf},
isbn = {1530},
journal = {Sciences-New York},
keywords = {learning},
pages = {1--59},
pmid = {22175947},
title = {{Semi-Supervised Learning Literature Survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.2352{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@article{woodworth1938experimental,
author = {Woodworth, Robert S and Schlosberg, Harold},
journal = {New York},
pages = {327--328},
title = {{Experimental Psychology Holt}},
year = {1938}
}
@article{thorndike1911,
author = {Thorndike, Edward Lee},
title = {{Animal Intelligence}},
year = {1911}
}
@article{Kober2013,
author = {Kober, J. and Bagnell, J. A. and Peters, J.},
doi = {10.1177/0278364913495721},
file = {:home/ruben/Documents/Mendeley/Kober, Bagnell, Peters/The International Journal of Robotics Research/Reinforcement learning in robotics A survey - 2013 - Kober, Bagnell, Peters.pdf:pdf},
isbn = {9783642276446},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {learning control,reinforcement learning,robot,survey},
number = {11},
pages = {1238--1274},
title = {{Reinforcement learning in robotics: A survey}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364913495721},
volume = {32},
year = {2013}
}
@book{friedman2001elements,
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
publisher = {Springer series in statistics Springer, Berlin},
title = {{The elements of statistical learning}},
volume = {1},
year = {2001}
}
@misc{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1989.1.4.541},
file = {:home/ruben/Documents/Mendeley/LeCun et al/Neural Computation/Backpropagation Applied to Handwritten Zip Code Recognition - 1989 - LeCun et al.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
number = {4},
pages = {541--551},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
volume = {1},
year = {1989}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555},
file = {:home/ruben/Documents/Mendeley/Chung et al/Unknown/Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling - 2014 - Chung et al.pdf:pdf},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@article{Michie1968,
abstract = {BOXES is the name of a computer program. We shall also use the word boxes to refer to a particular approach to decision-taking under uncertainty which has been used as the basis of a number of computer programs. Fig. 1 shows a photograph of an assemblage of actual boxes—matchboxes to be exact. Although the construction of this Matchbox Educable Noughts and Crosses Engine (Michie 1961, 1963) was undertaken as a 'fun project', there was present a more serious intention to demonstrate the principle that it may be easier to learn to play many easy games than one difficult one. Consequently it may be advantageous to decompose a game into a number of mutually independent sub-games even if much relevant information is put out of reach in the process. The principle is related to the method of subgoals in problem-solving (see Newell et al. 1960) but differs in one fundamental respect: subgoals are linked in series, while sub-games are played in parallel, in a sense which will become apparent. DECOMPOSITION INTO SUB-GAMES},
author = {Michie, D and Chambers, R A},
file = {:home/ruben/Documents/Mendeley/Michie, Chambers/Machine Intelligence 2/BOXES An Experiment in Adaptive Control - 1968 - Michie, Chambers.pdf:pdf},
journal = {Machine Intelligence 2},
pages = {137--152},
title = {{BOXES: An Experiment in Adaptive Control}},
year = {1968}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new ran- domized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart.},
annote = {Good overview on relu family,
don't include in thesis. Contains good refs.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:home/ruben/Documents/Mendeley/Xu et al/ICML Deep Learning Workshop/Empirical Evaluation of Rectified Activations in Convolution Network - 2015 - Xu et al.pdf:pdf},
journal = {ICML Deep Learning Workshop},
pages = {1--5},
title = {{Empirical Evaluation of Rectified Activations in Convolution Network}},
year = {2015}
}
@article{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
doi = {10.1007/11564096_32},
file = {:home/ruben/Documents/Mendeley/Riedmiller/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method - 2005 - Riedmiller.pdf:pdf},
isbn = {3540292438},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {317--328},
title = {{Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method}},
volume = {3720 LNAI},
year = {2005}
}
@article{Silver2015,
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Silver, David and Deepmind, Google},
eprint = {1507.04296},
file = {:home/ruben/Documents/Mendeley/Silver, Deepmind/Unknown/Deep Reinforcement Learning - 2015 - Silver, Deepmind.pdf:pdf},
title = {{Deep Reinforcement Learning}},
year = {2015}
}
@article{Bakker2001,
author = {Bakker, Bram},
file = {:home/ruben/Documents/Mendeley/Bakker/Nips 2001/Reinforcement Learning with Long Short-Term Memory - 2001 - Bakker.pdf:pdf},
journal = {Nips 2001},
title = {{Reinforcement Learning with Long Short-Term Memory}},
year = {2001}
}
@article{Narasimhan2015,
abstract = {In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against a baseline with a bag-of-words state representation. Our algorithm outperforms the baseline on quest completion by 54{\%} on a newly created world and by 14{\%} on a pre-existing fantasy game.},
archivePrefix = {arXiv},
arxivId = {1506.08941},
author = {Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
eprint = {1506.08941},
file = {:home/ruben/Documents/Mendeley/Narasimhan, Kulkarni, Barzilay/Emnlp/Language Understanding for Text-based Games Using Deep Reinforcement Learning - 2015 - Narasimhan, Kulkarni, Barzilay.pdf:pdf},
journal = {Emnlp},
pages = {10},
title = {{Language Understanding for Text-based Games Using Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1506.08941},
year = {2015}
}
@article{Bellemare2012,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
file = {:home/ruben/Documents/Mendeley/Bellemare et al/Unknown/The Arcade Learning Environment An Evaluation Platform for General Agents - 2012 - Bellemare et al.pdf:pdf},
pages = {253--279},
title = {{The Arcade Learning Environment: An Evaluation Platform for General Agents}},
url = {http://arxiv.org/abs/1207.4708 http://dx.doi.org/10.1613/jair.3912},
volume = {47},
year = {2012}
}
@article{VanDerMaaten2008,
abstract = {KNAW Narcis. Back to search results. Publication - - (2008). Pagina-navigatie: Main. Title, - - . Published in, Journal of Machine Learning Research, Vol. 9, No. nov, p.2579-2605.},
author = {{Van Der Maaten}, L J P and Hinton, G E},
doi = {10.1007/s10479-011-0841-3},
file = {:home/ruben/Documents/Mendeley/Van Der Maaten, Hinton/Journal of Machine Learning Research/Visualizing high-dimensional data using t-sne - 2008 - Van Der Maaten, Hinton.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing high-dimensional data using t-sne}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=7911431479148734548related:VOiAgwMNy20J},
volume = {9},
year = {2008}
}
@article{Schaul2016,
annote = {Nice background chapter on biological experience replay. Good links.

They have the same worries about overfitting so they make it stochastic. Exactly what I wanted to achieve.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.05952v4},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David and Deepmind, Google},
eprint = {arXiv:1511.05952v4},
file = {:home/ruben/Documents/Mendeley/Schaul et al/Unknown/Prioritized Experience Replay - 2016 - Schaul et al.pdf:pdf},
pages = {1--21},
title = {{Prioritized Experience Replay}},
year = {2016}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
annote = {The paper.

Good takeaway about RL in general: The data evolves as you go, it's not a fixed like ML, where the underlying distribution doesn't change.

RMSProp with minibatches of size 32. Get a paper on this.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/ruben/Documents/Mendeley/Mnih et al/NIPS/Playing Atari with Deep Reinforcement Learning - 2013 - Mnih et al.pdf:pdf},
issn = {0028-0836},
journal = {NIPS},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Jarrett2009,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({\&}gt; 65{\%}), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53{\%}).},
annote = {Referred by DeepMind's paper as a motivation for their convolutional architecture.},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
doi = {10.1109/ICCV.2009.5459469},
file = {:home/ruben/Documents/Mendeley/Jarrett et al/Proceedings of the IEEE International Conference on Computer Vision/What is the best multi-stage architecture for object recognition - 2009 - Jarrett et al.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2146--2153},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
annote = {Investigates capabilities of neural networks},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
file = {:home/ruben/Documents/Mendeley/Hornik/Neural Networks/Approximation capabilities of multilayer feedforward networks - 1991 - Hornik.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
number = {2},
pages = {251--257},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@phdthesis{Vaerenbergh2011,
author = {Vaerenbergh, Kevin Van},
file = {:home/ruben/Documents/Mendeley/Vaerenbergh/Unknown/Improving Clutch Engagement with Reinforcement Learning Toepassen van Reinforcement Learning ter Verbetering van het Schakelgedrag van T.pdf:pdf},
title = {{Improving Clutch Engagement with Reinforcement Learning Toepassen van Reinforcement Learning ter Verbetering van het Schakelgedrag van Transmissies}},
year = {2011}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
file = {:home/ruben/Documents/Mendeley/Graves/arXiv preprint arXiv1308.0850/Generating sequences with recurrent neural networks - 2013 - Graves.pdf:pdf},
isbn = {2000201075},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Schaul2015,
author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
file = {:home/ruben/Documents/Mendeley/Schaul et al/Proceedings of The 32nd International Conference on Machine Learning/Universal Value Function Approximators - 2015 - Schaul et al.pdf:pdf},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {1312--1320},
title = {{Universal Value Function Approximators}},
url = {http://jmlr.org/proceedings/papers/v37/schaul15.html},
year = {2015}
}
@article{Sharma2015,
abstract = {We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.},
archivePrefix = {arXiv},
arxivId = {1511.04119},
author = {Sharma, Shikhar and Kiros, Ryan and Salakhutdinov, Ruslan},
eprint = {1511.04119},
file = {:home/ruben/Downloads/action-recognition-using-visual-attention-nips2015.pdf:pdf},
journal = {arXiv preprint},
pages = {1--11},
title = {{Action Recognition using Visual Attention}},
url = {http://arxiv.org/abs/1511.04119},
year = {2015}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
annote = {So this paper seems to be written before the 2011 paper about Rectifiers. Should check it out, because if this one introduces Rectifiers I'm not sure what the point is of the other one.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:home/ruben/Documents/Mendeley/Nair, Hinton/Proceedings of the 27th International Conference on Machine Learning/Rectified Linear Units Improve Restricted Boltzmann Machines - 2010 - Nair, Hinton.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Maei2009,
abstract = {We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms effectiveness.},
annote = {Linked to by DeepMind's paper},
author = {Maei, H and Szepesvari, Csaba and Bhathnagar, S and Silver, D and Precup, D and Sutton, R},
file = {:home/ruben/Documents/Mendeley/Maei et al/Advances in Neural Information Processing Systems/Convergent temporal-difference learning with arbitrary smooth function approximation - 2009 - Maei et al.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
pages = {1--9},
title = {{Convergent temporal-difference learning with arbitrary smooth function approximation}},
year = {2009}
}
@techreport{Sutton,
author = {Sutton},
file = {:home/ruben/Downloads/SSR-98TR.pdf:pdf},
title = {{SSR-98TR.pdf}}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
annote = {NVidia uses this in their CuDNN implementation.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/ruben/Documents/Mendeley/Ioffe, Szegedy/arXiv/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - 2015 - Ioffe, Szegedy.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these con-trollers have limited memory and rely on being able to perceive the complete game screen at each deci-sion point. To address these shortcomings, this arti-cle investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recur-rent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with in-crementally more complete observations, DRQN's per-formance scales as a function of observability. Con-versely, when trained with full observations and eval-uated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learn-ing to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
annote = {Note: This paper contains a very good discussion on MDP/POMDP and includes nice visual reps of convolutional layers and Pong.

Exactly my first attempt at such an architecture: first post-convolutional layer is replaced with an LSTM layer.

DQRN only replicates performance and performs better on incomplete observations. Otherwise, it doesn't improve (according to Abstract).

They quote the DeepMind paper to not be able to find any games that are still only partially observable given 4 frames. However, I have a paper stating that some games need 18 frames. What's up with this?

Very nice approach to determining whether the obscured-trained DRQN can work well in non-obscured situations (to test for generalization). They do this by comparing activations between situations of p=.5 and p=1 (p=chance of obscuring). They differ only slightly, as opposed to the normal DQN.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
file = {:home/ruben/Documents/Mendeley/Hausknecht, Stone/arXiv preprint arXiv1507.06527/Deep Recurrent Q-Learning for Partially Observable MDPs - 2015 - Hausknecht, Stone.pdf:pdf},
journal = {arXiv preprint arXiv:1507.06527},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
year = {2015}
}
@article{Cottreil1991,
author = {Cottreil, G W},
file = {:home/ruben/Documents/Mendeley/Cottreil/Proceedings of the 1990 Summer School on Connectionist Models/Extracting Features From Faces Using Compesssion Networks Face, Identity, Emotion, and Gender Recognition Using Holons - 1991 - Cottreil.pdf:pdf},
journal = {Proceedings of the 1990 Summer School on Connectionist Models},
pages = {328--338},
title = {{Extracting Features From Faces Using Compesssion Networks: Face, Identity, Emotion, and Gender Recognition Using Holons}},
year = {1991}
}
@article{turing1948intelligent,
author = {Turing, Alan M},
journal = {The Turing Test: Verbal Behavior as the Hallmark of Intelligence},
publisher = {MIT Press},
title = {{Intelligent machinery, a heretical theory}},
volume = {105},
year = {1948}
}
@article{Williams1995,
abstract = {Recurrent connectionist networks are important because they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks that which have an independent temporal component not easily handled through the use of simple tapped delay lines. Some examples are tasks involving recognition or generation of sequential patterns and sensorimotor control. This report examines a number of learning procedures for adjusting the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples. The procedures are all based on the computation of the gradient of performance error with respect to network weights, and a number of strategies for computing the necessary gradient information are described. Included here are approaches which are familiar and have been first described elsewhere, along with several novel approaches. One particular purpose of this report is to provide uniform and detailed descriptions and derivations of the various techniques in order to emphasize how they relate to one another. Another important contribution of this report is a detailed analysis of the computational requirements of the various approaches discussed.},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1080/02673039508720837},
file = {:home/ruben/Documents/Mendeley/Williams, Zipser/Back-propagation Theory, architectures and Applications/Gradient-based learning algorithms for recurrent networks and their computational complexity - 1995 - Williams, Zipser.pdf:pdf},
isbn = {0-8058-1259-8},
journal = {Back-propagation: Theory, architectures and Applications},
pages = {1--45},
title = {{Gradient-based learning algorithms for recurrent networks and their computational complexity}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=kvbx6dVv2uEC{\&}oi=fnd{\&}pg=PA433{\&}dq=Gradient-Based+Learning+Algorithms+for+Recurrent+Networks+and+Their+Computational+Complexity+1+Introduction{\&}ots=MAFeylo232{\&}sig=-L3olgtrd3Do{\_}LZZ{\_}DQcwigyI},
year = {1995}
}
@inproceedings{kohavi1995study,
author = {Kohavi, Ron and Others},
booktitle = {Ijcai},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
volume = {14},
year = {1995}
}
@article{szepesvari2010reinforcement,
annote = {Book referred to as math text by the sutton book.},
author = {Szepesv{\'{a}}ri, Csaba},
journal = {Morgan and Claypool Publishers},
publisher = {Citeseer},
title = {{Reinforcement learning algorithms for MDPs}},
year = {2010}
}
@article{Moore1993,
abstract = {We present a new algorithm, Prioritized Sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Differencing and Qlearning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have difficulty.  1 Introduction  This paper introduces a memory-based technique, prioritized sweeping, which can be used both for Markov prediction and reinforcement learning. Current, model-free, learning algorithms perform well relative to real time. Classical meth...},
author = {Moore, Andrew W. and Atkeson, Christopher G.},
doi = {10.1.1.28.7241},
file = {:home/ruben/Documents/Mendeley/Moore, Atkeson/Machine Learning/Prioritized Sweeping Reinforcement Learning with Less Data and Less Real Time - 1993 - Moore, Atkeson.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
pages = {103--130},
title = {{Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.7241},
year = {1993}
}
@article{Sutton1998a,
abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
file = {:home/ruben/Documents/Mendeley/Sutton, Barto/IEEE Transactions on Neural Networks/Reinforcement Learning An Introduction - 1998 - Sutton, Barto.pdf:pdf},
isbn = {0262193981},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {intro to dynamic programming,ro to reinforcement learning},
number = {5},
pages = {1054--1054},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction}},
url = {http://dl.acm.org/citation.cfm?id=551283$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=712192},
volume = {9},
year = {1998}
}
@article{Silver2014,
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:home/ruben/Documents/Mendeley/Silver et al/Proc. of the 31st International Conference on Machine Learning/Deterministic Policy Gradient Algorithms - 2014 - Silver et al.pdf:pdf},
journal = {Proc. of the 31st International Conference on Machine Learning},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://jmlr.org/proceedings/papers/v32/silver14.html},
year = {2014}
}
@book{pavlov1927conditional,
author = {Pavlov, Ivan Petrovich},
publisher = {H. Milford},
title = {{Conditional reflexes: An investigation of the physiological activity of the cerebral cortex}},
year = {1927}
}
@article{Veness,
annote = {Peter gelinkt: over het BRUTE algoritme. Deterministische actie sequenties.},
author = {Veness, Joel and Bellemare, Marc G},
file = {:home/ruben/Documents/Mendeley/Veness, Bellemare/Unknown/AAAI-15 Workshop W18 - Unknown - Veness, Bellemare.pdf:pdf},
title = {{AAAI-15 Workshop W18}}
}
@article{Zhang2011,
abstract = {This paper proposes a hybrid crop classifier for polarimetric synthetic aperture radar (SAR) images. The feature sets consisted of span image, the H/A/$\alpha$ decomposition, and the gray-level co-occurrence matrix (GLCM) based texture features. Then, the features were reduced by principle component analysis (PCA). Finally, a two-hidden-layer forward neural network (NN) was constructed and trained by adaptive chaotic particle swarm optimization (ACPSO). K-fold cross validation was employed to enhance generation. The experimental results on Flevoland sites demonstrate the superiority of ACPSO to back-propagation (BP), adaptive BP (ABP), momentum BP (MBP), Particle Swarm Optimization (PSO), and Resilient back-propagation (RPROP) methods. Moreover, the computation time for each pixel is only 1.08 × 10(-7) s.},
author = {Zhang, Yudong and Wu, Lenan},
doi = {10.3390/s110504721},
file = {:home/ruben/Documents/Mendeley/Zhang, Wu/Sensors/Crop classification by forward neural network with adaptive chaotic particle swarm optimization - 2011 - Zhang, Wu.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Artificial neural network,Particle swarm optimization,Principle component analysis,Synthetic aperture radar},
month = {may},
number = {5},
pages = {4721--4743},
pmid = {22163872},
publisher = {Molecular Diversity Preservation International},
title = {{Crop classification by forward neural network with adaptive chaotic particle swarm optimization}},
url = {http://www.mdpi.com/1424-8220/11/5/4721/},
volume = {11},
year = {2011}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/ruben/Documents/Mendeley/Mnih et al/Nature/Human-level control through deep reinforcement learning - 2015 - Mnih et al.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Bruin2015,
abstract = {Recent years have seen a growing interest in the use of deep neural networks as function approximators in reinforcement learning. This paper investigates the po- tential of the DeepDeterministic PolicyGradientmethod for a robot control prob- lem both in simulation and in a real setup. The importance of the size and com- position of the experience replay database is investigated and some requirements on the distribution over the state-action space of the experiences in the database are identified. Of particular interest is the importance of negative experiences that are not close to an optimal policy. It is shown how training with samples that are insufficiently spread over the state-action space can cause the method to fail, and how maintaining the distribution over the state-action space of the samples in the experience database can greatly benefit learning.},
annote = {Paper has a good theoretical background, maybe interesting for thesis.
Takeaway: sample database needs to be and stay representative, otherwise previously learned can still be forgotten. Also interesting: Keeping oldest and newest samples works better than just twice the amount of newest samples. 

Interesting note on future work: better policy to replace samples in the db. Links to 19 and 20.

"It is shown how training with samples that are insufficiently spread over the state-action space can cause the method to fail, and how maintaining the distribution over the state-action space of the samples in the experience database can greatly benefit learning.

"When this is combined with the fact that neural networks can easily forget about data that they are no longer trained on [16], it becomes apparent that when an experience database of limited size is used, the contents should be maintained"

"Additionally, even if a network has previously learned to do a task well, it can forget this knowledge completely when learning a new task, even when the new task is related to the old one"

"Regularization can help prevent neural networks from over-fitting to their training data. However, the regularization of neural networks used for control is currently not as well understood as the reg- ularization of neural networks in other domains [17]. It is therefore important to ensure the training data in the experience database is varied enough for the neural networks to properly generalize their knowledge to the whole state-action space. A simplemethod that helps with this issue is introduced in Section 4."},
author = {de Bruin, Tim and Kober, Jens and Tuyls, Karl and Babuska, Robert},
file = {:home/ruben/Documents/Mendeley/Bruin et al/Unknown/The importance of experience replay database composition in deep reinforcement learning - 2015 - Bruin et al.pdf:pdf},
pages = {1--9},
title = {{The importance of experience replay database composition in deep reinforcement learning}},
url = {rll.berkeley.edu/deeprlworkshop/papers/database{\_}composition.pdf},
year = {2015}
}
@article{Hausknecht2014,
abstract = {This article addresses the challenge of learning to play many dierent video games with little domain- specic knowledge. Specically, it introduces a neuro-evolution approach to general Atari 2600 game playing. Four neuro-evolution algorithms were paired with three dierent state representations and evaluated on a set of 61 Atari games. The neuro-evolution agents represent dierent points along the spectrum of algorithmic sophistication - including weight evolution on topologically xed neural net- works (Conventional Neuro-evolution), Covariance Matrix Adaptation Evolution Strategy (CMA-ES), evolution of network topology and weights (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e. HyperNEAT) allow scaling to higher-dimensional representations (i.e. the raw game screen). Previous approaches based on temporal- dierence learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuro-evolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuro-evolution is a promising approach to general video game playing.},
author = {Hausknecht, Matthew and Lehman, Joel and Miikkulainen, Risto and Stone, Peter},
doi = {10.1109/TCIAIG.2013.2294713},
file = {:home/ruben/Documents/Mendeley/Hausknecht et al/IEEE Transactions on Computational Intelligence and AI in Games/A neuroevolution approach to general atari game playing - 2014 - Hausknecht et al.pdf:pdf},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Algorithms,artificial neural networks,evolutionary computation,genetic algorithms,neural networks},
number = {4},
pages = {355--366},
title = {{A neuroevolution approach to general atari game playing}},
volume = {6},
year = {2014}
}
@article{Y.2015a,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Y. and Bengio, Y. and Hinton, G.},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/ruben/Documents/Mendeley/Lecun, Bengio, Hinton/Nature/Deep learning - 2015 - Lecun, Bengio, Hinton.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84930630277{\&}partnerID=40{\&}md5=befeefa64ddca265c713cf81f4e2fc54},
volume = {521},
year = {2015}
}
@article{Glorot2011a,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
annote = {Linked to by 'Empirical Evaluation of Rectified Activations in Convolution Network': ". It is commonly believed that the superior performance of ReLU comes from the sparsity"},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:home/ruben/Documents/Mendeley/Glorot, Bordes, Bengio/Aistats/Deep Sparse Rectifier Neural Networks - 2011 - Glorot, Bordes, Bengio.pdf:pdf},
issn = {15324435},
journal = {AISTATS '11: Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
pages = {315--323},
title = {{Deep sparse rectifier neural networks}},
volume = {15},
year = {2011}
}
@article{Lin1992,
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
annote = {Linked to by "Experience Replay for Real-Time Reinforcement Learning Control" as the paper that introduces Experience Replay.},
author = {Lin, Long J.},
doi = {10.1007/BF00992699},
file = {:home/ruben/Documents/Mendeley/Lin/Machine Learning/Self-improving reactive agents based on reinforcement learning, planning and teaching - 1992 - Lin.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,planning,teaching},
number = {3-4},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
volume = {8},
year = {1992}
}
@article{Keerthi1994,
abstract = {This paper gives a compact, self-contained tutorial survey of reinforcement learning, a tool that is increasingly finding application in the development of intelligent dynamic systems. Research on rein-forcement learning during the past decade has led to the development of a variety of useful algorithms. This paper surveys the literature and presents the algorithms in a cohesive framework.},
author = {Keerthi, S Sathiya and Ravindran, B},
file = {:home/ruben/Documents/Mendeley/Keerthi, Ravindran/Unknown/A tutorial survey of reinforcement learn- ing - 1994 - Keerthi, Ravindran.pdf:pdf},
keywords = {Reinforcement learning,dynamic programming,neural networks,optimal control},
number = {6},
pages = {851--889},
title = {{A tutorial survey of reinforcement learn- ing}},
volume = {19},
year = {1994}
}
@article{Machado2015,
author = {Machado, Marlos C and Hausknecht, Matthew and Bellemare, Marc G},
file = {:home/ruben/Documents/Mendeley/Machado, Hausknecht, Bellemare/Unknown/Arcade Learning Environment Technical Manual (v.0.5.1) - 2015 - Machado, Hausknecht, Bellemare.pdf:pdf},
pages = {1--20},
title = {{Arcade Learning Environment Technical Manual (v.0.5.1)}},
year = {2015}
}
