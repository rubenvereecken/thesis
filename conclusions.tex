\chapter{Conclusions}
% While deep reinforcement learning offers a novel approach
% to learning control policies
% from raw input data such as pixels in images,
% doing so in partially observable environments
% still proves challenging for the current state of the art.

\paragraph{}
The problems under consideration
are games in the Arcade Learning Environment,
an Atari game emulator
which allows a learning agent to interact
with its environment.
The learning agent observes images as raw pixels
and must successfully learn a value function,
embodied by the network,
that indicates which actions are best in which states.
Along the way the network also has to learn
how to extract useful features
from these raw frames
in a hierarchic fashion
in order to do so.

\paragraph{}
The goal of this thesis was to improve the performance
of the standard Deep Q-Network
in the Arcade Learning Environment
by exploring different approaches
to learning dependencies over time.

DQN employs a straightforward frame stacking approach:
frames are always fed into the network
four at a time
in order to render the games Markovian,
meaning that any useful historic information
is present in this small stack
and nothing beyond could influence decisions.

We have started out by showing that this approach
is already an improvement over
using just a single frame
because important information such as object velocity is lost,
though the boost in performance is heavily dependent
on the game under consideration.

\paragraph{}
We have explored three alternative approaches
to learning dependencies between frames over time.
The first two of these employ a fixed history
to learn features that can be derived from short time spans
while the latter can learn to leverage information
observed an arbitrary amount of time before.

\paragraph{}
The Late Fusion network proposed here
is based on the Late Fusion architecture
that has already successfully been used
for action recognition in video,
though our variant is heavily based on DQN
for better comparison.
Instead of a single stack of convolutional layers,
the Late Fusion architecture contains
two such stacks, or towers, that each take as input a single frame
though both share weights in order to learn exactly the same features.
Only the fully-connected layer
that both towers feed into
has access to time information
because it can see
the difference between the high-level features
from both stacks.

The Late Fusion network
has a learning process slower than standard DQN
but still forms an improvement over using
just a single frame.

\paragraph{}
The 3D convolutional architecture can
learn features over time and space combined
at multiple hierarchical levels
because it gives more control over how
filters over time are learned and propagated throughout the network.
We have shown that it significantly outperforms standard DQN.
Moreover,
it is shown to vastly surpass any other approach
on the stochastic POMDP
Flickering Pong
given sufficient input frames.

\paragraph{}
Both of these architectures have access to an extra input dimension
compared to DQN because of the way they handle time.
While DQN converts images to grayscale before feeding them
into the network,
we have shown that making use of the extra dimension
to hold RGB color information
significantly boosts learning speed
of both architectures.

\paragraph{}
Lastly,
we have constructed an LSTM-based DQN architecture
with the goal in mind for it to learn how to retain
information from previous frames.
LSTM units keep internal cell state,
the use of which is regulated by
an input gate, forget gate and output gate
that all have to be trained.
They also contain a recurring connection.
This makes them ideal for learning over sequences
which reinforcement learning inherently exhibit
in its sequential nature.

While this approach does exhibit a learning curve,
it is considerably slower than all other considered approaches.

\section{Contributions}
\label{sec:contributions}
We have introduced the Late Fusion architecture,
originally from supervised learning,
in the reinforcement learning context
to learn value functions.

Additionally,
we have introduced a deep 3D convolutional network
capable of learning features over time
as alternative to the currently widely used 2D convolutional DQN.

Finally,
we have investigated an LSTM-based deep network
and a variant of the Deep Q-Learning algorithm
tailored to learning over sequences
and found it wanting in terms of learning speed
on the Arcade Learning Environment.

\paragraph{}
For all of these architectures,
we have considered multiple combinations of hyper parameters
for the Arcade Learning Environment
and tuned these to find the fastest learning variant.
For the 3D convolutional network,
this resulted in the network
that keeps a uniform time filter size of 2,
slowly merging time throughout the network.

For the LSTM-based network,
results were not as clear
though the network with 256 units and a gradient step of 30
fared better than both the variant with more units
and the variant with a smaller gradient step.

\paragraph{}
In addition to tuning,
all architectures considered here
give access to an extra image dimension,
the use of which we have tested with RGB colors
instead of the standard grayscale.
We have tried the Late Fusion
and 3D convolutional networks with this enriched input
and found this provided significant performance gains.

\paragraph{}
Finally,
because we found the ALE benchmark wanting
and needed a more clear example of a POMDP,
we have used Flickering Pong,
a stochastically heavily obscured variant of Pong,
as an additional benchmark
and found only
the 3D convolutional network
with sufficient frames
managed to learn decent performance
on this problem.

\section{Future Work}
\label{sec:future_work}
The experiments in this thesis
demonstrated how the LSTM architecture learned rather slowly
compared to the other architectures considered,
while the other architectures perform well
but can only learn short-term dependencies
because of their use of a fixed history window.
It may be interesting to combine the strengths of both approaches;
one could employ the short-term approaches suggested here
to learn features over small spans of time
- typically these are derived features such as velocity -
combined with recurrent units that operate
on abstract features
(such as is already attempted in this work)
which includes features over short periods of time.

\paragraph{}
Additionally,
we believe performance is somewhat hindered
by performing a cold restart on the LSTM layer's
internal cell states.
It may be interesting to learn initial values
for this internal state
so more of the training process carries over
between training episodes
instead of only gate weight updates.
