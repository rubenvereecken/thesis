\chapter{Machine Learning}

\section{Introduction}
The world of today is generating huge amounts of data
as it goes along,
and more and more of it is put to use.
It's quite probable that any consumer in our society
that goes shopping,
uses some kind of service,
or uses the internet
on a daily basis
performs actions that end up in a database.
Not only do persons generate data,
so does everything else we monitor,
and our society happens to monitor quite a lot.
This data is invaluable for any organization that benefits
from information,
and truly, most do.
It is used by financial institutions like insurance companies,
medical institutions, commercial companies, scientific research
and everything in between.
Evidently, as the need for knowledge (or awareness thereof)
increases, so must our abilities to uncover it,
making machine learning an incredibly interesting topic
in today's world.


People are good at creating hypotheses
about what things mean,
connecting the dots
and predicting cause or outcome.
Yet we can only cope
with so much data at a time
and without direction,
the answers we seek may elude us.
Enter the computing device,
programmable to our needs.
Computer programs often take the form
of our expert knowledge put to detailed writing
but they can also cover the gaps where
we don't have the knowledge
and instead need to uncover it.
This includes anything of valuable information to us,
be it patterns in certain data or hypotheses for predictions.
Where the domain expert describes such hypotheses
using a computer program,
the machine learning expert devises ways of uncovering
the hypotheses.
Evidently, the power of computers would be much greater
if they would be made able to learn as we do.

Great advances have already been made in the domain,
yet we have not managed to make computers learn
as well as humans do,
at least not in as many regards as we do.
The best applications do, however,
get better results than humans in very specific domains.


\paragraph{}
Machine learning is a collection of methods
used to bridge the aforementioned gap
between data and knowledge.
A machine learning algorithm learns from data
and learns an hypothesis or model on it,
which can then be used to make predictions on future data.
This separates it from other algorithms
designed by an expert
that follow a static rule set.
It is inherently an interdisciplinary field,
with its roots firmly embedded
in a statistical foundation
combined with AI,
drawing inspiration from fields such as
information theory,
complexity theory,
psychology
and other fields.

The rest of this chapter will describe the basics of machine learning
along with the specific tools I will use throughout this thesis.
The foundation should be quite sufficient to build up
an understanding of the field
so as to allow the reader to follow the following chapters comfortably.
Next to exploring a foundation,
I will describe specific methods such as neural networks
and convolutional layers,
both exciting techniques that will be used further on.
However, perhaps to the reader's delight,
a cursory glance at the high-level introduction of these methods
should suffice for the reader who only seeks to understand
the applications described herein.
The careful reader is of course cordially invited
to read the more detailed descriptions.


\section{Hands-on Introduction}
A machine learning algorithm
is usually required to form hypotheses
based on given observations.
In this way it can be seen as
a black-box oracle
in which data are shoved
and answers pop out.
Let's take as an easy example
the probably overused and dead-beaten weather forecast.

Let's say we only need to do a daily forecast.
Data come in in the form of meteorological observations
such as temperature, wind speed, and whatever else
experts observe to make top-notch predictions.
This then gets paired
with a description of the actual weather the next day.
Together they form experience-target pairs
$(x_i, y_i)$
that correspond to a meteorological observation
on the one day and the weather on the next day,
in the hope that we can somehow deduce the hidden
link between the two.
The target $y_i$ is what we're really after
so we should choose carefully as to what we are trying to predict.
$y_i$ can be a nominal value,
i.e. from a given set of values
such as the set $(sunny, rainy, windy)$,
in which case the problem is called a \textit{classification} problem.
Contrarily, it can be a real-valued number such as temperature
in which case we call the problem a \textit{regression} problem.
The distinction is worth noting because
some techniques lend themselves well to
one category but not necessarily to the other.
On this data we will try to learn a hypothesis $h$
that can predict the target weather $y$
for any given observation $x$.
Now all we need to do is fill in the bits in between.

We already have a vague description of what we want to achieve
but now we need to design the problem.
Let us pick for our targets the real-valued temperature on a given day.
The observations, or \textit{features}, we will base our model on
will take the form of a vector
$(Temperature, Sky, Wind)$ or $(x_1, x_2, x_3)$ short,
where $Temperature$ and $Wind$ are real-valued,
whereas $Sky$ takes on one of the three values
$(Sunny, Rainy, Cloudy)$.
Now we need some kind of function connecting
observation to target.
One of the easiest choices would be a linear
function that combines the features
and outputs the target we are looking for.
It would look quite simple:

$$ y = w_0 + w_1x_1 + w_2x_2 + w_3x_3 $$

We are still stuck with the problem that $x_2$, or $Sky$,
is a nominal value and therefore does not fit all too well with algebra.
We can work around this by agreeing on
a little preprocessing step
that maps $Sunny$ to $-1$, $Rainy$ to $0$ and $Cloudy$ to $1$.
Now that our model is defined, all that is left for us to do
is train it.

In order to train our model we need a data set with training examples
$(x_i, y_i)$,
where the temperature $y_i$ corresponds to
the observations $x_i$ the previous day.
All that remains now is to pick the best weights
so the resulting model is the best hypothesis.
We describe a \textit{best fit}
we need some metric of performance.
The easiest one available is the error on the predictions made by the model
compared to the targets in the training data.
The best hypothesis is the one that minimizes this error.
The most common error by far is the \textit{mean squared error},
though others certainly exist.
Given a training set $D$, it is defined as follows:

$$ E = \frac{1}{|D|} \sum_{(x_i, y_i) \in D}{(h(x_i) - y_i)^2} $$

Multiple algorithms exist that minimize $E$ in such a manner.
If all data is available in one go, as it is in our case,
we can simply apply linear regression to our system
of $|D|$ equations $y = xw$ in order to find a $w$
with the lowest error.

\subsection{Underfitting and Overfitting}
Why not just go for an error of $0$?
We could, however it is unlikely since we suffer from two problems:
insufficient data and a delusion of a perfect world.
In other words, our features probably are not sufficient
and our data is noisy.
If we still manage a training error of $0$
we probably committed the grave mistake of
\textit{overfitting} the training data,
meaning we included the noise of the data in our model.
The reason this is to be avoided is that noise
cannot be predicted by definition and a model that tries
to do so will perform even worse on similar unseen samples.

< TODO image of overfitting >

\subsection{Finishing up the example}
An alternative algorithm to find our optimal weights $w$
is the \textit{weight update rule},
a source of inspiration for many more algorithms.
The idea behind is that every sample
you calculate your prediction
and shift the weights in the direction of the error,
proportionally to the size of the error.
So, for every sample:
\begin{enumerate}
\item Calculate $\hat{y_i} = h(x_i)$
\item Update $w_i = w_i + \eta(y_i - \hat{y_i})x_i$
\end{enumerate}

The update size can be tweaked with $\eta$.

\paragraph{}
As you have witnessed yourself,
designing a machine learning task
is rather an art than an exact science.
The first optimization algorithm proposed,
linear regression,
is only available for linear models.
Finding the ultimate weights
makes more sense in a regression context
as opposed to a classification context.

There are many more such decisions to made
and models to choose from.
During the next sections,
I will touch on some interesting
models that are used throughout this thesis.

\section{Bias and Variance}
%TODO

\section{Curse of Dimensionality}
The distance between two instances is calculated based on all of their attributes.
For these instances to have a small distance between them,
they should be close to one another in all of their attributes,
or otherwise put,
they should be similar in all aspects.
The more attributes or dimensions they have,
the harder this becomes.
This informally described phenomenon we call
the \textit{curse of dimensionality}.

\begin{figure}[ht]
\center

\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{curse_3d.pdf}
  \caption{3 dimensions}
  \label{fig.ml.cod3}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{curse_2d.pdf}
  \caption{2 dimensions}
  \label{fig.ml.cod2}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{curse_1d.pdf}
  \caption{1 dimension}
  \label{fig.ml.cod1}
\end{subfigure}

\caption{Each plot contains 100 points
with values in all dimensions ranging
uniformly between 0 and 10.
The curse of dimensionality is especially clear
from the difference between the 2-dimensional
and 1-dimensional plot.
}
\label{fig.ml.cod}
\end{figure}

Figure ~\ref{fig.ml.cod} attempts a visual demonstration.
Each plot contains 100 instances with a variable amount of dimensions,
but with values for all dimensions ranging uniformly between 0 and 10.
As instances become increasingly more multi-dimensional, %TODO wording
the spaces containing them become increasingly more sparse.
Even though an instance may be close to another instance
in some dimensions,
any discrepancies in other dimensions
will only increase the distance between the two instances.

Let us resume with the above example.
Another way to uncover the increasing sparsity is to examine
is by examining the density of each space.
The 1-dimensional space has 10 instances per unit of space,
since it contains 100 instances spread over 10 units.
In contrast then, the 2-dimensional space contains
only a single instance per square unit of space
and the 3-dimensional space a forlorn $.1$ instance
per cubic unit of space.
In order to boost the density of the 3-dimensional space
to be on par with the 1-dimensional one,
we would need a hundred times as many instances
in this particular scenario,
for that is the factor between the two space sizes.

A final measure of sparsity of a space I like to think in
is the expected average distance for dimensions,
given their distributions.
In other words,
if one were to pick two instances from one of the above spaces uniformly,
what would the expected distance between the two be?
In this running example, %TODO verify this with someone
this measure is 5 for the 1-dimensional space
and approximately 7,07 and 8,66 for the other
two spaces.
This measure relates directly to the curse of dimensionality;
as your number of dimensions increase,
expect your distances to go up as well.

\paragraph{}
The phenomenon is often encountered in areas such as
text processing or image processing
where large numbers of dimensions are inevitable.
A naive model that tries to represent texts
could have a different dimension for each word,
likewise a model that describes an image
would have a separate dimension for each pixel,
easily reaching into thousands of attributes.

Such models would need a tremendous amount
of instances to have any sort of meaningful
distance measure.
Acquiring the required quantities of data
is often infeasible,
making techniques that shrink space dimensionality
very valuable in practice.
Techniques can range from discarding
least important attributes to
statistically combining attributes
to even learning new attributes that represent
higher-level concepts
from the lower-level attributes.
A popular example of the latter are
convolutional networks,
a topic I will cover in %TODO ref




\section{•}