\chapter{Reinforcement Learning}
\epigraph{
  Fool me once, shame on you \\
  fool me twice, shame on me.
}{Popular proverb}


\section{The Problem}
Reinforcement Learning in general is
the idea of learning from interaction with the environment;
a concept humans are familiar with albeit sometimes subconsciously.

The child learns to walk by attempts, failure,
and eventually success.
During every interaction with our environment
we are constantly aware of how it reacts to us,
be it when we walk down the street
or hold a conversation with someone.
We are even aware of animals doing the same.

\paragraph{}
The idea of trial-and-error learning has long been in play,
the term itself even used in the 19th century
to describe observations of animal behavior
\parencite{woodworth1938experimental}.

Edward Thorndike phrased the
\textit{Law of Effect}
which is,
after some of his own amendments,
still considered a basic principle
governing much of animal behavior.

\begin{displayquote}
Of several responses made to 
the same situation, those which are accompanied or closely 
followed by satisfaction to the animal will, other things being 
equal, be more firmly connected with the situation, so that, 
when it recurs, they will be more likely to recur; those which 
are accompanied or closely followed by discomfort to the ani- 
mal will, other things being equal, have their connections with 
that situation weakened, so that, when it recurs, they will be 
less likely to occur. The greater the satisfaction or discomfort, 
the greater the strengthening or weakening of the bond. 

\attrib{\cite{thorndike1911}, p. 244}
\end{displayquote}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{puzzlebox.png}
  \caption{
    A puzzle box used by Thorndike in his experiments.
    The hungry cat is locked inside the box which it can open
    by solving some kind of puzzle
    in order to reach the fish outside.
    Results showed that the cat went from solving the puzzle by sheer happenstance
    to methodically opening it as if by habit.
  }
  \label{fig:puzzlebox}
\end{figure}

The psychological field of animal behavior and learning
has a longer history than the computational counterpart does,
with popular examples such as Pavlov's research.
The Russian Nobel laureate studied how animals
responded to different stimuli:

\begin{displayquote}
It is pretty evident that under natural conditions the normal animal must
respond not only to stimuli which themselves bring immediate benefit or harm,
but also to other physical or chemical agencies—waves of sound,
light, and the like—which in themselves only signal the approach of these stimuli;
though it is not the sight and sound of the beast of prey 
which is in itself harmful to the smaller animal, but its teeth and claws.
\attrib{\cite{pavlov1927conditional}, p. 14}
\end{displayquote}

In this same text the term "reinforcement"
was used for the first time in the context of animal learning.

Pavlov is most known for his experiment with dogs.
He noticed dogs would produce saliva in response
to receiving food.
He then associated a secondary stimulus to the act
of feeding the animals
by ringing a bell beforehand.
The dogs learned to associate the bell
with food and produced saliva
upon hearing the bell,
even before or without receiving anything.

\paragraph{}
Using trial-and-error to achieve artificial intelligence 
was among the earliest ideas in the field.
Alan Turing describes positive and negative stimuli
to influence an algorithm:

\begin{displayquote}
  When a configuration is reached for which the action is undetermined, a
random choice for the missing data is made and the appropriate entry is
made in the description, tentatively, and is applied. When a pain stimulus
occurs all tentative entries are cancelled, and when a pleasure stimulus
occurs they are all made permanent.

\attrib{\cite{turing1948intelligent}}
\end{displayquote}

Reinforcement Learning as I treat it here
solely means the computational approach
to learning from interaction,
except where mentioned explicitly.
We will not theorize on animal behavior
or try to model it in order to create computational models.
Sometimes, however,
inspiration is drawn from animal behavior
but it usually is no more than that;
analogies only go as far as they serve us.
The perspective used here is that of the engineer,
not the neuroscientist.

\paragraph{}
Central to reinforcement learning is that a learner
interacts with its environment in order to learn
and ultimately aims to achieve some goal.
Applied to human behavior in the course of a lifetime,
we could say humans -in general- try to optimize happiness.
Applied to something less daunting than the human condition,
a sculptor may try to optimize beauty
or expressivity of a sculpture,
learning along the way how to do so in the best possible way.
It is this goal-based interaction that forms the core
to reinforcement learning.

\paragraph{}
Reinforcement learning can be characterized
by three distinguishing features,
setting it apart from other fields of learning

\begin{description}
  \item[Closed loop]
    In order to learn, a learning agent must interact
    with the environment to collect the necessary information.
    However, each action changes the environment in a certain way
    and in turn influences the agent's future inputs.
    This forms a \textit{closed loop}.

  \item[Discovery]
    An agent is provided with no instructions on what actions to take
    and how this will impact the environment.
    It is to discover this itself.

  \item[Time factor]
    Consequences of an action can be delayed by an unknown amount of time.
    Even the reward signal can be received many time steps later.
    The agent is to figure out for itself
    how its actions relate temporally to consequences.
    
    A good example of this is the game of chess with only a reward signal
    at the end of a match, either positive or negative.
    Some moves during the game are probably more important
    than others and some more complicated,
    like traps that take multiple moves to set up.
    Yet all moves together are rewarded with only a single signal
    at the end of the match.
    It is for the agent to unravel and attribute
    its actions according to importance.
\end{description}

\subsection{Reinforcement Learning and Machine Learning}
As I tried to convey above,
a crucial aspect to reinforcement learning
is trial-and-error, learning from interactions
with an environment that is not necessarily known.
This makes reinforcement learning different from
\textit{supervised learning}
which is most associated with machine learning.
In a supervised setting,
a labeled example set is provided
for the learner to learn from.
Learning in this context means generalizing from the training set
so queries about data not in this set
can still be answered accurately.

Reinforcement learning is different
in that it takes on more of the problem;
a learning algorithm is not presented with data
but instead must gather it by interacting with the environment.
In doing so it must also make a tradeoff between exploration and exploitation,
a characterizing element to reinforcement learning.
A learning agent can either \textit{exploit}
what it knows to be the best action in order to achieve its goal,
thereby possibly ignoring alternative routes of action
that would have resulted in better results,
or choose to \textit{explore}
what impact its actions have on the environment.
Neither strategy can be followed exclusively
if one wants to learn anything worthwhile,
a good combination of both is always needed.

\paragraph{}
On the "opposite" side of supervised learning
is we call unsupervised learning,
which is about finding hidden patterns in unlabeled data.
The supervision gap between the two obviously pertains
to whether the data has been created in a supervised manner.
An illusion is created that there are two sides to machine learning,
though reinforcement learning does not seem to fit either.
Reinforcement learning is effectively a third machine learning paradigm.

\subsection{Applications (TODO)}
Reinforcement learning has been applied in a multitude of domains,
not only the ones immediately called to mind when thinking
of autonomous agents like physical robots of all kinds,
though those have definitely gained a lot of attention.

%TODO pff geen zin nu
\paragraph{}
Survey by \cite{Kober2013}

\paragraph{}
Talk about self-driving cars

With the surge of new techniques and faster machines
reinforcement learning has also gotten popular with the public.
%TODO self-driving cars


\section{Reinforcement Learning Framework}
\subsection{Agent and Environment}
There are only few components to the reinforcement learning problem.
The learner and actor is dubbed the agent and everything outside it the environment.
The agent perceives its environment and acts on it,
receiving a reward in return.
The latter is a numerical value
which the agent tries to gain as much as possible of
during its time interacting with the environment.
This interaction goes on continually:
observation followed by action followed by reward.
The goal of the agent is to maximize its accumulated reward
over the entire span of a task,
i.e. an instance of a problem.
In order for it to do so
it must learn to not only look to immediate rewards
but must also look to what the future has to offer.

\begin{figure}[ht]
	\center
	\includegraphics[width=.8 \linewidth]{agent-environment.png}
	\caption{The agent interacts with the environment and 
  consequentially perceives a reward and the next state of the environment.
  \parencite{Sutton1998a}
  }
	\label{agent-env}
\end{figure}

\paragraph{}
Formally, we divide the problem into discrete time steps $t$.
A time step occurs every time the agent perceives a state $S_t$
from the set of possible states $\mathcal{S}$.
Based on this state the agent selects an action $A_t$
from its repertoire of actions $\mathcal{A}(S_t)$
available to it at in state $S_t$.
As a result, the next step it will receive
a numerical reward $R_{t+1} \in \mathcal{R}$
along with the next state $S_{t+1}$,
and so on and so on.

\paragraph{}
As you can see, the problem setting is entirely abstract
and can be filled in in various ways.
In fact, the same practical problem may be defined in different ways
depending on the goal.
A state could just as well represent a robot's raw sensor readings
as well as higher-level representations
such as whether it is looking at a green or red light.
We say that state is provided by the environment
even though one could argue a robot's sensors
are part of the agent.
Instead, we look at the agent as the observer and decision-maker
taking in everything from a distance,
even its physical body external to it
as part of the environment.
Similarly to states,
actions can range from raw low-level values
like motor voltages
to higher-level concepts like which room to go to
or whether to turn left or right.
Basically, actions are decisions taken by the distant observer,
for the designer to decide which shape they take.

\paragraph{Cart Pole Balancing Example}
A popular experiment in reinforcement learning
that has its roots close to half a century ago
is the cart pole balancing experiment
\parencite{Michie1968}.

In this experiment,
a pole is affixed to a joint to the cart.
Because of the joint,
the pole can move in a two-dimensional plane
as shown in Figure \ref{fig:cartpole}.
The cart can move in two directions
parallel to the joint's movement.

Let us say the pole starts in a perfectly orthogonal
position to the cart.
However, because of the joint and
inherent material imperfections, however microscopic,
the pole will start to drop toward one side or the other.
It is now the cart's goal
to keep the pole balanced perfectly upward
by moving however it can.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{cartpole.png}
  \caption{Cart Pole Experiment
  %TODO attribute to http://library.rl-community.org/wiki/CartPole_(Java)
  }
  \label{fig:cartpole}
\end{figure}

\paragraph{}
Applied to our framework,
we could at first glance say the cart is the obvious agent
and everything else its environment.
It would however make more sense
to name the textit{decision maker} that controls the cart the agent
and anything physical the environment or observable state.

As it is commonly defined,
a state exists of the angle of the pole with the cart
along with the angular velocity of the pole.
This would mandate three possible actions:
move left, move right, or stand still.

However, say the cart works in a more complicated manner
and actually has to accelerate instead of just
move at a fixed speed in one direction.
In that case,
the aforementioned state is insufficient
to correctly balance the pole;
car speed is now a crucial component
and should be added to the state if possible
to give a more complete description
and allow the agent to learn even better policies.
If not added, the agent will still learn
to the best of its abilities though.


\subsection{Goal and Rewards}
During a task,
an agent receives rewards upon acting with the environment.
Rewards are crucial to the reinforcement learning problem
as they are way the algorithm designer
can create goals for the agent (the \textit{what})
without stating how these goals might be achieved (the \textit{how}).
In the earlier example of a chess game,
you would reward the agent for winning
(and even penalize for losing)
but in general not reward specific steps the agent took
to reach its goal of winning.
That would be bringing domain specific knowledge into it,
which is not necessary and can
even be detrimental to the learner's success.
The most basic form contains no domain knowledge.


\paragraph{}
Rewards are numerical and can be positive or negative,
real-valued or restricted to natural numbers,
the agent just has one goal with them:
to somehow accumulate it.
There are different ways to define such a goal.

\begin{description}
  \item[finite horizon] 
    The most naive way to collect rewards is through the
    \textit{finite-horizon} model,
    in which the agent aims to optimize its expected
    rewards for the next fixed amount of steps.
    This is rarely appropriate and still leaves us
    with the challenge of finding an appropriate value
    for the constant horizon window.
  \item[infinite horizon]
    The alternative then is to not have a fixed window
    but try to maximize reward over the rest of the agent's lifespan,
    albeit discounted so later rewards have less impact than current ones.
  \item[average reward]
    The average reward model makes the agent optimize
    its average reward over the rest of its lifetime.
    It relates closely to the infinite-horizon model, without the discount.
\end{description}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{optimality.png}
  \caption{Different goal models
  \parencite{Kaelbling1996} 
  }
  \label{fig:optimality}
\end{figure}

We will only use the \textit{infinite-horizon} model.
Over the course of a lifetime
the agent will try to optimize

the expected total discounted return
\begin{equation}
  E\left( \sum^{\infty}_{t=0}\gamma^tr_t \right)
\end{equation}

or put otherwise,
at timestep $t$ the agent will try to optimize his
total discounted reward $G_t$
by choosing the appropriate action $A_t$:


\begin{equation}
  G_t = \sum^{\infty}_{k=1}\gamma^kR_{t+k}
\end{equation}

As stated, we will discount later rewards
to avoid infinity as time goes on.
This discounting is tuned by $\gamma \in [0,1]$,
the \textit{discount rate}.
Though a nifty mathematical trick,
it also has the interpretation of giving more weight
to rewards closer in the future than those
further down the road.
In case of $\gamma = 1$,
an reward later on has the same impact on the goal as an equally high reward
encountered earlier.
Tuning $\gamma$ effectively tunes the agent's
farsightedness, with lower values for $\gamma$
make for a `greedier' agent.

\paragraph{Cart Pole Balancing Example (cont.)}
Let us take again the cart pole balancing example.
We still have to split up the problem in discrete time steps.
Take for example 40 steps every second,
because that is the limitation imposed by the hardware
and it does not make sense to make decisions faster
than they can be acted upon.

\paragraph{}
We now need to convey to the cart agent exactly 
what we want it to achieve:
to keep the pole balanced upward indefinitely.
A good way to get the goal across is to define a range
within which the robot is doing a good job,
then always assign $+1$ whenever the pole is inside that range
and $-1$ when the pole drops too low.
This will make sure the agent tries to get
within the positive-reward range
but potentially also as far as possible
from the negative-reward fringes,
depending on how the agent learns.
If the positive range is then defined around the center,
as it should be,
the pole should be balanced perfectly upward.


\section{Markov Decision Processes}
The Markov property is an interesting property for a problem to have
because it states that all relevant information at some point
is present in a state, a state being whatever is available to the agent.
This means that the whole history leading up to a certain point,
insofar that it is relevant,
is encoded in the state at that time. 
Such a state is said to be Markov.

\paragraph{}
Consider the probability of ending up in a state $S_{t+1}$
with reward $R_{t+1}$
after performing some action $A_t$
in a state $S_t$.
This probability is denoted

\begin{equation}
  Pr(S_{t+1}=s', R_{t+1}=r | S_t, A_t)
\end{equation}

We would say a state signal is Markov if this probability is the same as

\begin{equation}
  Pr(S_{t+1}=s', R_{t+1}=r | S_t, A_t, S_{t-1}, A_{t-1},..., S_0, A_0)
\end{equation}

which is the same scenario except given the entire state and action history.
If these two are indeed equal
then the history is effectively encoded in the current state $S_t$.

\paragraph{}
Markov states are handy because we do not need the whole history
in order to make good decisions or even the best decisions.
The best policy in function of the entire history
is the same one as without the history
because everything relevant to predicting the environment
is already encoded in the current state.
This is especially important for reinforcement learning
as decisions will always be made in function of only the current state.
Even when the state is not entirely Markov,
it is easier for us to think of it as Markov
or an approximation thereof
because it gives us a framework on which
we can build an understanding,
so long as we are conscious of possible deviations.

\subsection{Markov Decision Processes}
\label{sub:markov_decision_processes}
Once a reinforcement learning task has the Markov property
we call it a \textit{Markov Decision Process} or MDP for short. 
Such an MDP is completely defined by its probability
of transitioning to any state $s'$ with reward $r$
when starting in any state $s$ and acting with some action $a$:

\begin{equation}
  p(s', r|s, a) = Pr\{S_{t+1}=s', R_{t+1}|S_t=s, A_t=a\}
\end{equation}

And if one would like to know the chance of transitioning to a state
regardless of the reward associated to the transition
(the \textit{state-transition probability})
it could be easily calculated as:

\begin{equation}
  \begin{split}
    p(s'|s, a)
    &= Pr{S_{t+1}=s'|S_t=s, A_t=a} \\
    &= \sum_{r \in \mathcal{R}} p(s', r|s, a)
  \end{split}
\end{equation}

Another especially useful number is the expected reward
for a certain state-action pair:

\begin{equation}
  \begin{split}
    r(s, a)
    &= E[R_{t+1} | S_t = s, A_t = a] \\
    &= \sum_{r \in \mathcal{R}}r \sum_{s' \in \mathcal{S}} p(s', r|s, a)
  \end{split}
\end{equation}

As should be obvious from these examples,
the Markov assumption
is an extremely convenient one.

\subsection{Partially Observable Markov Decision Processes}
\label{sub:partially_observable_markov_decision_processes}
One approach to non-Markovian settings
are \textit{Partially Observable MDP's} (POMDP).
They can be seen as generalizations of Markov decision processes
with incomplete information
\parencite{lovejoy1991survey}.

Formally, we can say there is an underlying MDP to the POMDP
with states $\mathcal{S} = \{s1, s2,\dots,s_N\}$
and a transition probability as described above.
The agent can however not observe a state directly,
at time step $t$ it instead observes an observation $o_t$
that relates to $s_t$ in some way,
according to some unknown probability distribution $P(o|s_t)$.

\paragraph{}
One way that POMDP's arise often is because of noisy sensors,
so observations do not completely accurately reflect true state. 
Another example is when state features can be derived
from observable features.
A typical example of such features are derived features
such as speed of an object as it relates to the object's position.
Speed is often a crucial component but it is not always
part of a state and indeed one could argue it should not be
because it is derived from another part of the state, over time.

\paragraph{}
Many approaches to POMDP's
include building some kind of model of the environment
so the relation between observation and hidden state can be used
\parencite{Cassandra1994}.
Other approaches use some kind of history window,
both fixed
\parencite{mccallum1995instance}
and variable-width windows
\parencite{ring1994continual}
have been attempted.

In section %TODO ref section recurrent
we will explore another road.

\paragraph{Cart Pole Balancing Example (cont.)}
In the cart pole example, we originally said we could have both angle
and angular velocity of the pole as part of the state.
What of the scenario when we do not supply angular velocity to the agent,
instead it must rely only on the current angle?

The agent does not know how fast the pole swings,
not even in which direction.
All it knows is the current angle.
Ideally it would know the direction of the pole
so it could counteract or complete its movement,
but in order to do that it must now look at the history
of the pole: which direction does it come from?
What is the distance travelled since last step?
This is clearly not Markovian,
as in the markovian case all necessary knowledge
is encoded in the current state,
whereas this scenario requires knowledge of at least the previous step.
Indeed, if the previous state could be part of the current state
the agent would already have a wealth of information.
We have:

\begin{equation}
\begin{split}
Pr\{S_{t+1}=s', R_{t+1}| S_t = s, A_t = a\} \\
\neq Pr\{S_{t+1}=s', R_{t+1}| S_t = s, A_t = a, S_{t-1} = s'', A_{t-1}=a''\} 
\end{split}
\end{equation}

This fixed-window approach is exactly what has been attempted successfully by
\citeauthor{Lin1992a} (\citeyear{Lin1992a}),
though they state it only works in cases
where the window is sufficient to encompass the required history.

\section{Value Functions}
The \textit{policy} of an agent is the decision maker,
the component of the algorithm that decides on the action in each state.
For a state $s \in \mathcal{S}$
and an action $a \in \mathcal{A}(s)$,
$\pi(a|s)$ is the probability of taken action $a$ in state $s$
as directed by the policy $\pi$.

\paragraph{}
We now introduce the notion of a \textit{value function};
a function that estimates how good a certain state is for the agent.
Recall that the agent will try to optimize discounted accumulated reward $G_t$,
this in turn depends on the future actions of the agent.
Since actions are a result of the policy $\pi$,
we always define a value function in terms of a policy:

\begin{equation}
  \begin{split}
    v_\pi(s) &\equiv E_\pi[G_t | S_t = s] \\
    &= E_\pi \left[ \sum^\infty_{k=1}\gamma^kR_{t+k} \middle| S_t = s \right]
  \end{split}
\end{equation}

We call this the \textit{state-value function} for policy $\pi$.

\paragraph{}
Then there is the \textit{state-action function} for policy $\pi$,
denoted $q_\pi(s,a)$,
which is the expected return when taking action $a$ in state $s$
then following policy $\pi$:

\begin{equation}
  \begin{split}
    q_\pi(s,a) &\equiv E_\pi[G_t | S_t = s, A_t = a] \\
    &= E_\pi \left[ \sum^\infty_{k=1}\gamma^kR_{t+k} \middle| S_t = s, A_t = a \right]
  \end{split}
\end{equation}

Note that I still have not said how to calculate these functions.
As they are defined here, they are the mathematical entities we aim to achieve
and give us a way of thinking in ideal terms
that we can then later approach in practice.

\paragraph{}
There is an extremely important recursive relation
between value functions that forms the foundation for the sections to come.

\begin{equation}
  \begin{split}
    v_\pi(s)
    &= E_\pi \left[ \sum^\infty_{k=1}\gamma^kR_{t+k} \middle| S_t = s \right] \\
    &= \sum_{\substack{s' \in \mathcal{S} \\ a \in \mathcal{A}(s)}}
    \pi(a|s) p(s'|s, a) \left [ r + \gamma v_\pi(s') \right ]
  \end{split}
\end{equation}

We call this relation the \textit{Bellman equation}.
Broken down, it is the sum of expected returns for each state $s'$
accessible from state $s$,
weighted by the chance of ending up in state $s'$
(which is of course impacted by both the odds of picking action $a$
and ending up in state $s'$ after).

\paragraph{}
Using value functions,
we can define a partial ordering between policies
in order to decide on the best one.
One policy is better than the other
if its expected return is greater than the other's.
Formally,

$$
\pi \geq \pi' \iff v_\pi(s) \geq v_{\pi'}(s) \, \forall s \in \mathcal{S}
$$

We now have a front of \textit{optimal policies},
which we will denote by $\pi*$.
They all share the \textit{ optimal state-value function }
and \textit{optimal action-value function}:

\begin{align*}
  v_*(s) &\equiv \max_{\pi} v_\pi(s) & \forall s \in \mathcal{S} \\
  q_*(s, a) &\equiv \max_\pi q_\pi(s,a) & \forall s \in \mathcal{S},
  \forall a \in \mathcal{A}(s)
\end{align*}

Finally, we can write the optimal action-value function
in function of the optimal state-value function.

\begin{equation}
  q_*(s,a) = E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
\end{equation}

\section{Temporal Difference Learning}

\section{Q-Learning}
Often we will discount future rewards
by a certain factor $\gamma$ per time step,
resulting in a total future discounted return
for the agent of
$R_t = \sum{T}{t'=t} \gamma^{t'-t}r_{t'}$,
with T the last time step.
This return is dependent on some policy $\pi$.
We now define $Q*(s,a)$ as the maximum expected return
when in a state s and taking action a.
This assumes we already know the optimal policy
in every subsequent state.
More formally,
$Q*(s,a) = max_{\pi}E[R_t|s_t=s, a_t=a, \pi]$

\paragraph{}
Since we assume we already know the optimal action in every state,
$$Q*(s,a) = E_{s'~\mathcal{E}}[r+\gamma max_{a'}Q*(s',a')|s,a]$$

Q-learning uses this equation directly
by plugging it into an iterative update.
Every step, the known Q-value for the visited state-action pair
is updated using a simple update rule:

$$<<TODO Q-learning update rule>>$$

with $\alpha$ a discount factor to stabilize the update.
This learned action	-value function directly approaches
the optimal action-value function $Q*$,
meaning that it always learns the optimal policy
of taking action $a' = argmax_{a}Q(s,a)$
in any state s.
The policy that is used to learn these Q-values
is only used for exploration purposes such that
sufficient state-action pairs are visited
and updated.
However, the policy itself is not optimized.
We call this off-policy learning,
as opposed to on-policy learning such as Sarsa-TD
where Q-values are updated with respect to
the currently used policy.
An advantage of Q-learning
is that it is guaranteed to converge
with probability 1
once certain conditions are fulfilled,
such as sufficiently small step size $\alpha$.

\paragraph{}
A problem with this learning approach is that
it enumerates all state-action pairs,
without generalization beyond visited pairs
and the added disadvantage of having to visit
each state-action pair multiple times
in order to have the Q-values converge.
Note that this makes the exploration strategy especially important,
as an agent that starts with a default Q-value assumption of 0
for each state-action pair can easily overfit
to non-zero Q-values if it is too greedy.
Moreover, and most importantly,
this approach does not scale well beyond only the smallest and most trivial
of state-action spaces.
This because the agent won't be able to enumerate all state-action pairs
sufficiently in such scenarios.
In the next section I will discuss how to alleviate this problem.

\section{Function Approximation}
To combat the problems brought along by a large state-action space,
we can use function approximators instead of storing Q-values
for each state-action pair,
which would quickly become infeasible.
To this end,
we could use any approximation method commonly used in machine learning
to approximate our state-action values.
However, reinforcement learning makes some methods
less suitable than others.
Samples gathered iteratively using reinforcement learning
usually have high correlation since an agent
takes a certain path through
the state-action space,
where neighboring states are usually somewhat similar.
The target to be learned is also not stationary,
since the real target value is often now known
but bootstrapped with some estimation.
For example,
in Q-learning the optimal action-value function $Q*$
is approximated using $r+\gamma max_{a'}Q(s',a')$
since we do not know the optimal $Q*$ in advanced.
This substituted target is still changing
because it has not been learned fully yet
and is both the value to be learned
as the target used for learning.
In subsequent sections I will discuss
how we can minimize the impact of these problems.
Still, it is important to be aware of them
when designing a function approximator.

\paragraph{}
One way of looking at the error of a Q-value approximator
with parameters $\theta$ is to look at the Mean Squared Error
between between the maximum expected return and
the predicted state-action value for all state-action pairs:
% formula kinda from sutton
% https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node86.html
$$MSE(\theta) = \sum_{s \in S}[Q*(s,a)-Q(s,a,\theta)]^2$$
This error can then to be minimized over all samples
by a method such as gradient descent.

That is the traditional machine learning approach.
Note that reaching a global optimum for the parameters
is often only possible for linear functions
and not so anymore for more complex nonlinear functions.
In the harder case, we could instead look for local optima,
i.e. a case when $MSE(\theta') < MSE(\theta)$
for all $\theta$ near $\theta'$.

One should also be careful with simply minimizing the MSE
of the state-action function approximator
as our ultimate goal is to have the best policy $\pi$,
% TODO so.. do they collide? Sutton was vague at first	
two goals that do not necessarily collide.


\section{Eligibility Traces}
Valt te overwegen.

\section{Actor-Critic}
