\chapter{Experiments and Results}\label{chap:experiments}% TODO maybe get rid of this section\section{Problem Statement}\label{sec:problem_statement}In the Deep Q-Network section (\ref{sec:dqn})I described an approach to deep reinforcement learning.While the approach works well,it has not managed to beat all games under considerationnor would it generalize well to just any gamethat fits the input description.Much care has been taken to make DQN as generic as possible,yet one detail stands out.A single state consists of four consecutive image framesin order to make the problem more Markovian,a useful trait because it allows us to rely onuseful theoretical properties that have been establishedfor Markov settings.However, not all games carry sufficient informationin the last four frames alone.Some need a few more,while others could conceivablyshow informationthat will then be needed to act correctlyafter a long period of time has passed.In other words, there could be hidden statethat the agent nevertheless had access toat some point in the past.\paragraph{}My goal is now to explore different approachesto this fixed-window history approach,to explore different ways of dealing with time.Ideally,the learning technique should not employ an approachthat contains a fixed window anywhere at all.However, such techniques are considered here as well.% TODO discuss POMDP\paragraph{}The rest of this chapter is organized as follows.First, I will examine the Arcade Learning Environmentin close detail in so far as this benefitsthe following sections.Then, I will closely investigate three approachesto the time problem.The three techniques discussed in this thesis are\textit{Late Fusion},\textit{3D Convolutions}and \textit{LSTMs}.Each in turn will be discussed,then explored and evaluted.Finally,I will draw the conclusions of this thesisin \ref{sec:conclusions}.\section{Arcade Learning Environment}\label{sec:arcade_learning_environment}In order to fully understand the experiments that followand the implications of their results,it is important to first have a closerlook at the Arcade Learning Environment\parencite{bellemare13arcade}.ALE is built on top of Stella\footnote{http://stella.sourceforge.net},an open-source Atari emulator.It enables one to programmatically interactwith any compatible Atari game file.This includes getting the screen in raw pixels,access to the Atari's emulated RAM,the game's current scoreand actually interacting with the game by sending actions.\paragraph{}To appreciate the setting completely we will need toinvestigate the hardware the games considered here used to run on.The Atari 2600 was released in 1977.Its CPU ran at 1.19 Mhz,more than a 1000 times slowerthan a single average core used for personal computers nowadays.The size of the RAM was especially small compared towhat is used in modern days, with 128 bytes.Of special interest to us is the console's graphic component.The screen is 210 by 160 pixelsand could display up to 128 colors.The Atari 2600 console is interacted with usinga joystick and a button.The joystick can move in 8 directions.Combining that,along with pushing the buttonwhile moving the joystick(or not moving it)gets us to 17 distinguishable actions.Add one for not doing anything at alland we have a total of 18 actions for our learning agent.% TODO consider getting rid of this,% just thought it was nice to see wtf it is\begin{figure}[h]\center\begin{subfigure}[t]{.5\textwidth}  \centering  \includegraphics[width=\textwidth]{ntsc_palette.png}  \vspace{.1\baselineskip}  \caption{    The Atari 2600 NTSC Palette used by many games.    It allows 3 bytes for luminiscence    and 4 bytes for chrominance,    resulting in the 128 distinct colors    you see here.  }  \label{fig:nips_network}\end{subfigure}\hfill\begin{subfigure}[t]{.4\textwidth}  \centering  \includegraphics[width=\textwidth]{atari.jpg}  \vspace{.1\baselineskip}  \caption{    The Atari 2600 with its joystick.  }  \label{fig:nature_network}\end{subfigure}\caption{}\label{fig:dqn_networks}\end{figure}\subsection{Shortcomings}\label{sub:shortcomings}While the Atari's computational simplicity,small screenand relatively straightforward gamesmake it a great testbed for reinforcement learning,the same characteristicsbring along a few shortcomingsthat bear discussing.\paragraph{}First off, the Atari 2600is entirely deterministic.% TODO really want a ref on thisThis allows some games to be abusedby learning how to exploit bugsor otherwise causing situationsthat would be very hard for a human to replicate,yet that a learning algorithm could easily managein a deterministic setting.This exploitation of a game's weak pointsdoes not sound bad on its own- after all, the agent is learning -but it is obviously a case of overfittingwhich should preferably be avoided.% TODO should I mention in DQN?DQN tries to avoid this by adding a random amountof nullops at the start of the turn,that is, the agent waits a random amount of turnsbefore it can play.\paragraph{}The console's determinism makes it sothat given the agent's history,the next state given an actioncan be known exactly.However,very few games actually need more thana few frames of historyin order to achieve this Markov property.Since the main interest of this thesisis dealing with time,long-term time dependencies would be especially interesting to investigate.Sadly, however good of a testing environment ALE may seem,it lacks thoroughly in this regard.% TODO erase this if you don'tI will discuss later how to circumvent this lack of complexity partially.\paragraph{}% TODO(final) adjust if neededI will now discuss two games in some detailand shed light on their core distinctionsin order to later understand and explainthe differences between their learning results.\subsection{Space Invaders}\label{sub:space_invaders}\subsection{Pong}\label{sub:pong}\subsection{General Setup}\label{sub:general_setup}All experiments that follow will be based on the DQN architecturedevised by \cite{Mnih2013}.\section{Stacked Frames Approach}\label{sec:stacked_frames_approach}In order to get a better feelingof why the dimension of time in reinforcement learningmerits closer examination and further research,this section will elaborate on the current approachas described by\citeauthor{Mnih2013} (\citeyear{Mnih2013}).\citeauthor{Mnih2013}found that for most games under discussion4 frames of history is sufficient to render the game approximately Markovian.This means that when considering not only a single framebut also the 3 prior to it,all information that could ever be required to make an optimal decisionat that point is encompassed within this small fixed window.\paragraph{}In order to combine these frames,it is sufficient to make sure each image frame only has a single image channel(e.g. grayscale or luminiscence)so the image depth dimension can be reusedto simply stack the frames on top of each other.This limitation is present because 2D convolutional networkscan only handle three dimensions,of which over the third one has no control in terms of filter size;it always reaches across the whole depth.This way of stacking frames along with the rest of the standard DQNis depicted in Figure \ref{fig:nips_network2}.\begin{figure}[htpb]  \centering  \includegraphics[width=0.8\linewidth]{nips_network2.pdf}  \caption{DQN architecture by \cite{Mnih2013}.    Note that the frames can not have any depth because of    the limitations imposed by 2D convolutions.  }  \label{fig:nips_network2}\end{figure}In order to figure out\begin{figure}[htpb]  \centering  \includegraphics[width=\linewidth]{stacked_vs_single.pdf}  \caption{    Rewards over time for 4 stacked frames as input    compared to just a single one    for five different games.  }  \label{fig:stacked_vs_single}\end{figure}\section{Late Fusion Network Approach}\label{sec:late_fusion_network_approach}The first alternative to the standard DQNconsidered here isthe Late Fusion DQN architectureinspired by \citeauthor{Karpathy2014} (\citeyear{Karpathy2014})who successfully deployed it foraction recognition in video clips.The base premise for this architectureis that in order to capture time dependenciesit is sufficient to look at differencesbetween frames.Presumably,this works better with short-term dependenciesas differences between frames close to each other in timeare smaller.Also, Late Fusionconsiders two frames with a fixed distance in time in between.This can only yield good results for longer time dependenciesif the dependency always spans approximately the same amount of time,a requirement that is undesirableand not always satisfiable.\paragraph{}The Late Fusion DQN is based on the default DQN implementation.In fact, the learning algorithm is entirely the sameand parameters are as depicted in Table \ref{tab:base}.Instead of a single stack of convolutional layers,we now have two towers that each take as input a single frame.The frames are separated by a fixed amount of framesthat go unused for the pass.Each of the two towers has no access to the framethat is input to the other one.Rather, the towers get combined by a fully connected layerthat is also the first of the layers that has accessto both time steps.Since each convolutional network now only has a single frame as input,we suddenly get back the image channel input dimension (depth)that was previously used to stack frames together.In order to use that dimension for time,DQN uses a grayscalewhich only requires a single scalar valueand as such this channel dimension was implicit before.Now that we have access to an extra input dimensionwe can again use color channelssuch as RGB,which might add useful information to the model.\paragraph{}The general gist to the Late Fusion architectureis that the convolutional layerscompute high-level featureswhich the fully-connected layer can then comparein order to compute the correct output.The full architecture is depicted in Figure \ref{fig:late_fusion}.\begin{figure}[htpb]  \centering  % \captionof{figure}{Late Fusion Network}  \includegraphics[width=0.8\linewidth]{late_fusion.pdf}  \caption{    Late Fusion DQN.    A sample of $n$ consecutive frames is chosen    but only the outer two frames    feed into the network.    The features learned by the two convolutional networks    is combined through the fully-connected layer    that combines both.    Only from the fully-connected layer up    is information from two different time steps available.  }  \label{fig:late_fusion}\end{figure}\subsection{Tuning}\label{sub:late_fusion_tuning}\section{3D Convolutional Network Approach}\label{sec:3d_convolutional_network_approach}Building on the idea that time is an extra spatial dimensionand should not be handled differentlyfrom image width or height,I investigate 3D convolutional layers next.In the original DQN implementationthe time dimension gets flattened immediatelyby the first convolutional layerand is not present explicitlyin its output;only the first convolutional layerhas explicit access to timebecause the 2D convolutional layer implementationextends along the entire depth of the input.Intuitively this means that only low-levelfeatures over time can be learnedwhereas we can easily imaginemore abstract featuresthat relate to time.This is where the 3D convolutional network comes in,in order to allow us to specify step size and stride across timein order to control how time is propagated through the entire network.\paragraph{}The original 2D convolution setup in DQNis exactly the same as a 3D convolutionwith 4 input frames and only a single image channel (grayscale),where the filter over time spans across all 4 input frames,as illustrated by Figure \ref{fig:conv2d_time}.A 3D convolutional layer gains an extra dimensionwhich can be used for image depth, like colors.It also allows us to specify the filter size and stride over time.Figure \ref{fig:conv3d_time} for exampleemploys a time filter size of 2 and a stride of 1.Since this comes down to combining every consecutive pair of frames,the output of the convolutional layer has a time dimensionthat is shrunk by 1 compared to the number of input frames.\begin{figure}[!htpb]  \begin{subfigure}[t]{.45\textwidth}    \centering    \includegraphics[width=.8\textwidth]{conv2d_time.pdf}    \caption{      2D convolution for a single feature as used by \cite{Mnih2013}.      Since time is treated as image depth,      a feature always spans across all time frames.    }    \label{fig:conv2d_time}  \end{subfigure}  \hfill  \begin{subfigure}[t]{.45\textwidth}    \centering    \includegraphics[width=.8\textwidth]{conv3d_time.pdf}    % \vspace{.1\baselineskip}    \caption{      3D convolution for a single feature      (depth slice of the convolutional layer)      with 4 input frames      and a time filter size of 2.      Connections of the same color depict shared weights.    }    \label{fig:conv3d_time}  \end{subfigure}  \caption{    Convolutions over time.  }  \label{fig:conv3d}\end{figure}\paragraph{}The 3D Convolutional DQNis again based entirely on the original DQN implementation\parencite{Mnih2013},with parameters in table \ref{tab:base}.An example instantiation of the 3D convolutional DQNcan be found in Figure \ref{fig:conv3d_network}.It leaves the amount if input channels unspecified;in our case this can either be 1 for grayscaleor 3 for RGB values.There are again two convolutional layers.Since these make use of 3D convolutions,filters over time can be specified.Both layers in Figure \ref{fig:conv3d_network}use a time filter size of 2,so the time dimensions shrinks by 1 after each.The fully-connected layerstill has a time dimension of size 2 to cope with in this example,as opposed to a flattened time dimension in the original DQN architecture.\begin{figure}[htpb]  \centering  \includegraphics[width=\textwidth]{conv3d_network.pdf}  % \vspace{.1\baselineskip}  \caption{    3D convolutional DQN.    The input consists of 4 frames    that can each have $C$ image channels.    Both convolutional layers employ a time filter size of 2,    meaning a single weight corresponds to    two frames of the layer's input.    Combined with a time stride of 1,    the time dimension of the output of each consecutive layer    is 1 smaller than its input.  }  \label{fig:conv3d_network}\end{figure}\subsection{Tuning}\label{sub:conv3d_tuning}\section{Long Short-Term Memory Approach}\label{sec:long_short_term_memory_approach}The last approach I consider involvesthe use of Long Short-Term Memory,a specific kind of recurrent neural network% TODO consider citationthat is capable of learning time dependenciesover arbitrary and unknown amounts of time.The very same architecture I will discuss herehas been attempted by\citeauthor{Hausknecht2015} (\citeyear{Hausknecht2015})who found favorable results for some few games.\citeauthor{Hausknecht2015}concluded that few significant differenceswere found with the original DQN architectureand that no general benefit could be gained comparedto simply stacking frames.One could however arguethat merely getting rid of the fixed windowwhich stacking automatically impliespresents a merit in its own rightand allows for better generalization of the architectureto other problem domains.\paragraph{}Recall that the LSTM unit as we consider it herecontains three \textit{gates}and an internal cell state (Section \ref{sec:lstm}).The input gate and forget gate together governthe content of the internal state,which forms the memorizing core of the unit,whereas the output gatelearns how to output the internal statein order to create the gate's output.\paragraph{}The intuitive idea behind the use of LSTMsis that the units will learn to remember important eventsand then use that information at some later timewhen it is required.Given their goal,they are especially useful in Partially Observable Markov Decision Processessuch as we have when considering only one Atari frame at a time.Sadly, Atari games generally only have very short time dependencieswhich make them an imperfect testbed forlong-term dependency learningso we will focus on learning those instead.\begin{figure}[htpb]  \centering  \includegraphics[width=.85\linewidth]{lstm_network.pdf}  % TODO peter should I call this DQRN  \caption{LSTM DQN}  \label{fig:lstm_network}\end{figure}\subsection{Design and Training}\label{sub:lstm_design_and_training}The Long Short-Term Memory variantof the deep Q-network- or Deep Recurrent Q-Network,as named by \ref{Hausknecht2015} -is once again based on the DQN versionby \ref{Mnih2013},with common parameters in Table \ref{tab:base}.\paragraph{}The network is constructed by taking outthe first fully-connected layerand replacing it with an LSTM layerwith the same or different amount of units.Everything else except the input stays the same.Instead of stacking frames along the depth dimension of the input,only one frame at a time now gets fed to the network.It is up the LSTM layer to handle memorization across time.Just like with the previous two approaches,ridding ourselves of the stacked frames opens up the input depth dimensiononce againto be used for image channels like RGB values.The network architecture is displayed in Figure \ref{fig:lstm_network}.\paragraph{}In the original Deep Q-Learning algorithm,after each action selectiona mini-batch learning update would occur.LSTMs require input frames to be observed in chronological order, however.This learning approach would not workbecause data gathering steps would be interleaved with mini-batch updates,breaking the required flow of time,so the LSTM unitswould consider the mini-batch to follow the observed frame right before the updatewhich is of course not the case.On top of that, mini-batches contain random samplesin order to break correlationbetween consecutive frameswhile LSTMs, again, require uninterrupted sequences.\paragraph{}In order to accomodate the new requirements,I adapted the Deep Q-Learning algorithmto keep sequential information intact at all times.First off,this required a new sampling method.Instead of sampling 32 random samples,I now sample \textit{random sequences}.This is done by selecting a single random time step,then including consecutive time stepsuntil either the batch size (by default 32)is reached or the episode terminates.This batch then gets treated like it would be normally,feeding it into the learning update while keeping the order intactso the LSTM's internal state could follow along.\paragraph{}This brings me to the internal state reset and backup.At the start of any episode,the LSTM's internal state along with its output historygets reset to initial values.These initial values can be fixed and suppliedby the network designeror even learned dynamically.The reset is required to make clear to the LSTMthat the first state of an episodedoes not follow the terminal state of the last episode.A parameter \textit{backup} and subsequent reseton the other handoccur at the start of every mini-batch learning update.This is done so the sequential mini-batchforms an isolated sequence of eventsand no accidental information from before the update gets carried into the updateby the LSTM's internal memory.After the updatethe internal state and output historyare restored from the backupso the agent can go on gathering dataas if no interruption had just occurred.\subsection{Tuning}\label{sub:lstm_tuning}\section{Conclusions}\label{sec:conclusions}