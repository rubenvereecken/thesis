\chapter{Extended Tables}
\label{app:a}

\begin{table}[]
  \vspace{-2cm}
\captionof{table}{Basic experiment hyper parameters}
\label{tab:base}
% \begin{tabularx}{p{.2\linewidth}l p{.7\linewidth}}
  \renewcommand{\arraystretch}{1.15}
\small
\begin{tabularx}{1.1\linewidth}{ll X}
Hyperparameter                  & Value             & Description                                                                                              \\
  \hline
mini-batch size                 & 32                & Number of random samples for each stochastic gradient descent update                                     \\
replay memory size              & $10^6$            & Replay memory contains this number of most recent frames for SGD updates                                 \\
agent history length            & 4                 & Number of most recent frames used as input to the Q network                                              \\
target network update frequency & 2000*             & Every 2000 parameter updates the target network gets updated with the parameters of the current network. \\
discount factor                 & 0.95              & Discount $\gamma$ used in Q-learning update                                                              \\
frame skip                      & 4                 & The agent only sees every 4th frame                                                                      \\
update frequency                & 1                 & Parameter updates occur every action selection                                                           \\
learning rate                   & $2 \cdot 10^{-4}$ & Learning rate $\eta$ used by RMSProp                                                                     \\
RMS decay                       & 0.99              & Gradient moving average decay factor $\rho$ for RMSProp                                                  \\
RMS epsilon                     & $10^{-6}$         & RMSProp small stability value added to denominator (see \ref{eq:rmsprop})                              \\
clip delta                      & false             & Whether to clip the delta; \cite{Mnih2015} clips this to $[-1, 1]$                                      \\
initial exploration             & 1                 & Initial value for $\epsilon$ in $\epsilon$-greedy                                                        \\
final exploration               & 0.1               & Smallest value for $\epsilon$ in $\epsilon$-greedy                                                       \\
epsilon decay                   & $10^{-6}$         & $\epsilon$ decays linearly over this amount of actions                                                   \\
replay start size               & 100               & Wait with training until the replay memory contains at least this amount of frames                       \\
max no-ops                      & 10**              & Start with a random amount of no-ops that is at most this amount                                         \\
resize frame method             & scale***          & Processing step employed to resize frames in order to reduce amount of pixels                            \\
\end{tabularx}
\paragraph{}
  The values in this table are based entirely on the DQN implementation by
  \cite{Mnih2013},
  except for those annotated.

  \textbf{*} The original implementation employs no separate target network.
  This is added in for stability.

  ** The original implementation uses no initial no-ops.
  This is added in to add some stochasticity to the games.

  *** The original implementation crops out the required area from a frame.
  This does not generalize well to games where important information
  falls outside of this frame.
\end{table}

\begin{table}
  \captionof{table}{\textit{Boost} hyper parameters}
  \label{tab:boost}
  \renewcommand{\arraystretch}{1.3}
\begin{tabularx}{1.\linewidth}{ll X}
Hyperparameter                  & Value             & Description                                                                                              \\
  \hline
discount factor                 & 0.99              & Discount $\gamma$ used in Q-learning update                                                              \\
update frequency                & 4                 & Parameter updates occur every action selection                                                           \\
replay start size               & 50000               & Wait with training until the replay memory contains at least this amount of frames                       \\

\end{tabularx}

\paragraph{}
\small
The rest of the parameters not present here are as supplied by Table \ref{tab:base}.
The combination in this table is gotten from \cite{Mnih2015},
which tended to work well for some games and considerably sped up training
because less updates are performed.

It is sufficient to draw quick comparisons.

\end{table}
